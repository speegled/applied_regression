# Solutions

## Chapter 2 Solutions

**Exercise 2.1**

a. We have that $\overline{X}$ is normal with mean $\overline{\mu}$ and variance $\frac 1{25}\sum \sigma_i^2$, i.e. $\overline{X} \sim N(3, \sigma^2 = 2.2)$. 

b. To confirm this via simulations:

```{r}
sim_data <- replicate(10000, {
  x <- rnorm(5, 1:5, sqrt((1:5)^2))
  mean(x)
})
mean(sim_data) #cf 3
var(sim_data) #cf 2.2
```

```{r}
plot(density(sim_data))
curve(dnorm(x, 3, sqrt(2.2)), add = T, col = 2)
```

**Exercise 2.2**
```{r}
bp.obese <- ISwR::bp.obese
sse <- function(beta) {
  sum((bp.obese$bp - (beta[1] + beta[2] * bp.obese$obese))^2)
}

optim(par = c(0,0), fn = sse)
```

Our line of best fit is $y = 96.8 + 23 x$.  Confirm with `lm`:

```{r}
lm(bp ~ obese, data = bp.obese)
```

For part (b), either answer is OK. The data seems pretty good to me, but I can see where some people didn't think so. As long as you say why.

**Exercise 2.3**

a. Note that $\sum_{i = 1}^n \beta_0 = n\beta_0$. The solution is $\hat \beta_0 = \overline{y} - \overline{x}$.

b. $\hat \beta_0$ is normal with mean $\beta_0$ and variance $\sigma^2/n$.

c. Let's simulate.

```{r}
xs <- runif(20, 0, 10)
sim_data <- replicate(10000, {
  ys <- 2 + xs + rnorm(20, 0, 1)  
  mean(ys) - mean(xs)
})
plot(density(sim_data))
curve(dnorm(x, 2, 1/sqrt(20)), add = T, col = 2)
```

**Exercise 2.4**

We have that 
\[
\frac{\hat \beta_1 - \beta_1}{\sqrt{nS^2/(n\sum x_i^2 - (\sum x_i)^2)}} \sim t_{n -2}
\]
So we compute.

```{r}
dd <- HistData::Galton
mod <- lm(child ~ parent, data = dd)
summary(mod)
```

The estimate for $S$ is given as 2.239, which we use. We compute the test statistic:

```{r}
(0.64629 - 1)/sqrt(2.239^2/(sum(dd$parent^2) - 1/928 * sum(dd$parent)^2))
```
The test statistic is -8.596837. We need to compute the likelihood of obtaining something that unlikely given that $H_0$ is true.


```{r}
pt(-8.596837, 926) * 2
```

With this $p$-value, we reject the null hypothesis. We conlcude that the slope is not 1. With more work, we can see that this means (roughly) that parents who are short or tall will tend to have children that are again short or tall, but less so than the parents.  Does that mean that eventually there will be little variation among heights of humans. Why or why not?

**Exercise 2.5**

a. An unbiased estimator for $S^2$ is 
\[
\frac{1}{n-1} \sum_{ i = 1}^n \bigl(y_i - \hat\beta_0 - x_i\bigr)^2
\]

b. Standard normal.

c. $t$ with $n-1$ degrees of freedom.

d. 
```{r, eval=FALSE}
dd <- read.csv("https://stat.slu.edu/~speegle/Spring2020/4870/data/problem_2_5.csv")
```

```{r, echo=FALSE}
dd <- read.csv("data/problem_2_5.csv")
```


```{r}
beta_0 <- mean(dd$ys) - mean(dd$xs)
s <- sqrt(1/(nrow(dd) - 1) * sum((dd$ys - beta_0 - dd$xs)^2))
test_stat <- (beta_0 - 2)/(s/sqrt(nrow(dd)))
test_stat
pt(test_stat, nrow(dd) - 1, lower.tail = FALSE) * 2
```

Note that this is the same as a paired $t$-test, where we have as our null hypothesis that $y_i - x_i$ is normal with mean 2 and unknown standard deviation. 

```{r}
t.test(dd$ys, dd$xs, mu = 2, paired = T)
```

**Exercise 2.6**

a. 
```{r}
dd <- carData::Davis
plot(dd$repwt, dd$weight)
```

b.
```{r}
mod_lm <- lm(weight ~ repwt, data = dd)
mod_huber <- MASS::rlm(weight ~ repwt, data = dd, psi = MASS::psi.huber)
mod_tukey <- MASS::rlm(weight ~ repwt, data = dd, psi = MASS::psi.bisquare)
plot(dd$repwt, dd$weight)
abline(mod_lm, col = 2) #red
abline(mod_huber, col = 3) #green
abline(mod_tukey, col = 4) #blue
```


## Chapter 3 Solutions

1. Let's load the data.

```{r}
secher <- ISwR::secher
plot(secher[,-4])
mod <- lm(bwt ~ bpd + ad, data = secher)
summary(mod)
```
Looks like a pretty good model already. Let's check the diagnostics.

```{r}
plot(mod)
```

Could be some skewness there. Let's see if a transformation might help the normality of the residuals.

```{r}
MASS::boxcox(mod)
```

This suggests a log transform of the response would improve the normality of the residuals. Finally, let's put `no` back in to the model. We hope that it is not strongly significant, as that might be an indicator of some sort of experimental flaw.

```{r}
mod2 <- lm(bwt ~., data = secher)
summary(mod2)
```
Note that `no` is not significant, and the Adjusted R-squared decreased. Both of these are what we would like to see.

