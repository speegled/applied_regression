# Classical Theory {#classicaltheory}

## Review {#review}

We will need several facts and techniques from MATH/STAT 3850 that you may or may not remember. The most important theory for this chapter is that related to *normal* random variables. 

Recall, a **normal** random variable with mean $\mu$ and standard deviation $\sigma$ has pdf given by
\[
f(x) = \frac {1}{\sqrt{2\pi}\sigma} e^{-(x- \mu)^2/(2\sigma^2)}
\]
We will not work much with the pdf directly, but rather use the built in R function `dnorm`, which has usage `dnorm(x, mu, sigma)`. Let's look at a couple of plots of pdfs of normal random variables.

```{r}
curve(dnorm(x, 0, 1/sqrt(2)), from = -3, to = 3, col = 3,
      main = "Normal rvs", 
      ylab = "likelihood") #green
curve(dnorm(x, 0, 2), add = T, col = 2) #red
curve(dnorm(x, 0, 1), add = T, col = 1) #black
```

Recall that the *area* under the curve between $x = a$ and $x = b$ of the pdf of $X$ gives $P(a \le X \le b) = \int_a^b f(x)\, dx$. Probability density functions can also be used to find the *expected value* of a random variable $X$ via
\[
\mu_X = E[X] = \int_{-\infty}^\infty x f(x)\, dx.
\]

```{theorem, label = "normalfacts"} 
Key facts about normal random variables.

1. If $X_1$ and $X_2$ are independent normal random variables with means $\mu_1, \mu_2$ and standard deviations $\sigma_1, \sigma_2$, then $X_1 + X_2$ is normal with mean $\mu_1 + \mu_2$ and standard deviation $\sqrt{\sigma_1 + \sigma_2}$.

2. If $X \sim N(\mu, \sigma)$ and $a$ is a real number, then $X + a \sim N(\mu + a, \sigma)$.

3. If $X \sim N(\mu, \sigma)$ and $a$ is a real number, then $aX \sim N(a\mu, a\sigma)$.

```

The key things to remember are that the sum of independent normals is again normal, and that translating or scaling a normal is still normal. The means and standard deviations of the resulting random variables can be computed from general facts about sums of rvs, given below.

```{theorem, label = "meanfacts"}
The following are facts about expected values of rvs.

1. $E[X + Y] = E[X] + E[Y]$, whether $X$ and $Y$ are independent or dependent!
  
1. $E[a] = a$.

1. $E[aX] = aE[X]$.

1. $E[\sum_{i = 1}^n a_i X_i] = \sum_{i = 1}^n a_i E[X_i]$.

  
```

Recall that the variance of an rv is $V(X) = E[(X - \mu)^2] = E[X^2] - \mu^2$, and that the standard deviation is the square root of the variance.

```{theorem, label = "variancefacts"}
Here are some key facts about variances of rvs.

1. $V(aX) = a^2 V(X)$

1. $V(\sum_{i = 1}^n a_i X_i) = \sum_{i = 1}^n a_i^2 V(X_i)$, *provided* that the random variables are independent!
  
1. $V(a) = 0$
  
```

The other key thing that you will need to remember is about simulations. We will be doing quite a bit of simulation work in this chapter.

### Estimating Means and standard deviations

Let's recall how we can estimate the *mean* of a random variable given a random sample from that rv. Recall that `r` + `root` is the way that we can get random samples from known distributions. Here, we estimate the mean of a normal rv with mean 1 and standard deviation 2. Obviously, we hope to get approximately 1.

We first get a single sample from the rv, then we put it inside `replicate`. 

```{r}
one_sample <- rnorm(1, mean = 1, sd = 2)
one_sample

sim_data <- replicate(10000, {
  one_sample <- rnorm(1, mean = 1, sd = 2)
  one_sample
})
```

This creates a vector of length 10000 that is a random sample from a normal rv with mean 1 and sd 2. For simple examples like this, it would be more natural to do `sim_data <- rnorm(10000, 1, 2)`, but the above will be easier to keep track of when we get to more complicated things, imho.

Next, we just take the mean (average) of the random sample.

```{r}
mean(sim_data)
```

And we see that we get pretty close to 1. Let's verify that $2 *X + 3 * Y$ has mean $2\mu_X + 3 \mu_Y$ in the case that $X$ and $Y$ are normal with means 2 and 5 respectively.

```{r}
x <- rnorm(1, 2, 1) #chose sd = 1
y <- rnorm(1, 5, 3) #chose different sd just to see if that matters
2 * x + 3 * y

```
That is one sample from $2X + 3Y$. Let's get a lot of samples.

```{r}
sim_data <- replicate(10000, {
  x <- rnorm(1, 2, 1) #chose sd = 1
  y <- rnorm(1, 5, 3) #chose different sd just to see if that matters
  2 * x + 3 * y
})
mean(sim_data)
2 * 2 + 3 * 5
```
We compare the estimated mean to the true mean of 19, and see that we are pretty close. To be sure, we might run the simulation a couple of times and see whether it looks like it is staying close to 19; sometimes larger, sometimes smaller.
 
To estimate the variance of an rv, we take a large sample and use the R function `var`, which computes the sample variance. This is not as straightorward from a theoretical standpoint as taking the `mean`, but we will ignore the difficulties. We know from Theorem \@ref(thm:variancefacts) that the variance of $2X + 3Y$ shoule be $4 * 1 + 9 * 9 = 85$.  Let's check it out. We already have built our random sample, so we just use `var` on that.

```{r}
var(sim_data)
```

Let's repeat a few times.

```{r}
sim_data <- replicate(10000, {
  x <- rnorm(1, 2, 1) #chose sd = 1
  y <- rnorm(1, 5, 3) #chose different sd just to see if that matters
  2 * x + 3 * y
})
var(sim_data)
```

Seems to be working. To estimate the standard deviation, we would take the square root, or use the `sd` R function.

```{r}
sd(sim_data)
sqrt(var(sim_data))
sqrt(85)
```

The reason it is tricky from a theoretical point of view to estimate the standard deviation is that, if you repeated the above a bunch of times, you would notice that `sd` is slighlty off on average. But, the variance is correct, on average. 

### Estimating the density of an rv

In this section, we see how to estimate the pdf of an rv and compare it to the pdf of a known distribution. We continue with the normal examples. Let's verify that $2X + 3Y$ is normal, when $X \sim N(0, 1)$ and $Y \sim N(1, 2)$. ($Y$ has standard deviation 2).

```{r}
sim_data <- replicate(10000, {
  x <- rnorm(1, 0, 1)
  y <- rnorm(1, 1, 2)
  2 * x + 3 * y
})
plot(density(sim_data))
```

The `plot(density())` command gives an estimated density plot of the rv that has data given in the argument of `density`. Let's add a pdf of a normal rv with the correct mean and standard deviation to the above plot, and see whether they match. The correct mean is $2 * 0 + 3 * 1$ and the sd is $\sqrt{4 * 1 + 9 * 4}$

```{r}
plot(density(sim_data))
curve(dnorm(x, 3, sqrt(40)), add = T, col = 2) #shows up in red
```

Alternatively, we could have used the simulated data and estimated the mean and sd.

```{r}
plot(density(sim_data))
curve(dnorm(x, 
            mean(sim_data), 
            sd(sim_data)), add = T, col = 2) #shows up in red
```

Since the estimated pdf of the simulated data matches that of the `dnorm` (more or less), we conclude that the distributions are the same. Note that if there is a discontinuity in the pdf, then the simulation will not match well at that point of discontinuity. Since we will mostly be dealing with normal rvs, we'll cross that bridge when we get to it.

```{exercise}
Let $X_1, \ldots, X_5$ be normal random variables with means $1, 2, 3, 4, 5$ and variances $1, 4, 9, 16, 25$. 

1. What kind of rv is $\overline{X}$? Include the values of any parameters.
  
1. Use simulation to plot the pdf of $\overline{X}$ and confirm your answer to part 1.

```

One last concept that we will run across a lot is *independence*. 
Two random variables are independent if knowledge of the outcome of one gives no probabilistic information about the outcome of the other one. 
Formally, $P(a \le X \le B|c \le Y \le d) = P(a \le X \le b)$ 
for all values of $a, b, c$ and $d$. 
It can be tricky to check this, and it is often something that we will **assume** rather than prove. 
However, one fact that we will use is that if $X$ and $Y$ are independent, then $X + a$ and $Y + b$ are also independent. 

## Linear Regression Overview

We present the basic idea of regression.
Suppose that we have a collection of data that is naturally grouped into ordered pairs, $(x_1, y_1), \ldots, (x_n, y_n)$. 
For example, we could be measuring the height and weight of adult women, or the heights of fathers and their adult sons. 
In both instances, these data would naturally be grouped together into ordered pairs, and we would likely be interested in the **relationship** between the two variables.
In particular, we would not want to assume that the variables are independent.

In the R package `HistData`, there is a data set that is the heights of parents and their adult children. Let's take a look.

```{r message=FALSE, warning=FALSE}
galton <- HistData::Galton
library(tidyverse)
ggplot(galton, aes(parent, child)) +
  geom_jitter(alpha = .5)
```

It appears that there is a positive correlation between the height of a parent and the height of their adult child. (The parent value is the average of the father and mother of the child., and the child value has been adjusted for sex of the child. So, this may not be the best example of rigourous data collection!)

```{r}
ggplot(galton, aes(parent, child)) + 
  geom_jitter(alpha = .5) +
  geom_smooth(method = "lm")
```

The model that we are using for this data is
\[
y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]
where the $\epsilon_i$ are independent normal random variables with mean 0 and unknown variance $\sigma^2$. 
The $\beta_0$ and $\beta_1$ are unknown **constants**. 
The only randomness in this model comes from the $\epsilon_i$, which we think of as the error of the model or the residuals of the model. 

In order to estimate the values of the coefficients $\beta_0$ and $\beta_1$, we want to choose the values that minimize the total error associated with the line $y = \beta_0 + \beta_1 x$. There are *lots* of ways to do this! Let's begin by looking at the error at a single point, $y_i - (\beta_0 + \beta_1 x_i)$. This is the *observed* value $y_i$ minus the $predicted$ value $\beta_0 + \beta_1 x_i$. If we are trying to minimize the total error, we will want to take the absolute value or something of the error at each point and add them up. Here are a few ways we could do this.

\begin{align}
SSE(\beta_0, \beta_1) &= \sum_{i = 1}^n \bigl(y_i - (\beta_0 + \beta_1 x_i)\bigr)^2\\
L_1(\beta_0, \beta_1) &= \sum_{i = 1}^n \bigl|y_i - (\beta_0 + \beta_1 x_i)\bigr|\\
E_g(\beta_0, \beta_1) &= \sum_{i = 1}^n g(y_i - \beta_0 + beta_1 x_i)
\end{align}

where $g$ is a *loss function* such that $g(x) = g(-x)$ and $g(x) \le g(y)$ whenever $0 \le x \le y$. Here are a few more loss functions $g$ that are commonly used.

```{r}
curve(x^2, from = -4, to = 4, col = 1,
      main = "Common Loss Functions") #black
curve(abs(x), add  = T, col = 2) #red
delta <- 2
curve(delta^2 *(sqrt(1 + (x/delta)^2) - 1),
      add = T,
      col = 3) #green
curve(robustbase::Mchi(x, cc = 1.6, psi = "bisquare"),
      add = T, 
      col = 4) #bue
```

The idea is that we want to minimize the sum of the loss function evaluated at the differences between the observed and the predicted values. We will see in later sections how to do that via math, but we also have R, which is good at doing things like this. 

Let's look at a different data set. This is the world happiness data set. People were surveyed in various countries around the world and asked to imagine the best life possible for them as a 10, and the worst life possible as a 0. Then they were asked to assign a number to their current life between 0 and 10. The results of the survey are summarized [here](https://mathstat.slu.edu/~speegle/Spring2020/4870/data/world_happiness_data.xls).

```{r}
dd <- readxl::read_xls("../data/world_happiness_data.xls") #This won't work for you!
dd <- filter(dd, Year == 2018) %>% 
  janitor::clean_names()
dd <- select(dd, country_name, life_ladder, log_gdp_per_capita)
ggplot(dd, aes(x = log_gdp_per_capita, y = life_ladder))  + 
  geom_point()
```

Let's get rid of the data with missing values.

```{r}
dd <- dd[complete.cases(dd),]
```

Now, let's find values of $\beta_0$ and $\beta_1$ that minimize the SSE. (We'll do the other loss functions below.)

```{r}
sse <- function(b0, b1) {
  sum((dd$life_ladder - (b0 + b1 * dd$log_gdp_per_capita))^2)
}
```

The minimization function that we are going to use requires the sse function to be in a format where it accepts a single *vector* of values.

```{r}
sse_one_argument <- function(b) {
  sse(b[1], b[2])
}
```

Now, we can call `optim` to *optimize* the SSE (which minimizes it).

```{r}
optim(par = c(1,1), fn = sse_one_argument)
```

We see that the intercept is about -1.03 and the slope is about 0.705. Let's plot that on the scatterplot.

```{r}
ggplot(dd, aes(x = log_gdp_per_capita, y = life_ladder))  + 
  geom_point() +
  geom_abline(slope = 0.705, intercept = -1.03) +
  geom_smooth(method = "lm")
```

Yep. That's the line that minimizes the sum of squares error, all right!



## Linear Regression with no intercept

We will present the theory of linear regression with no intercept. 
You will be asked as an exercise to repeat the arguments for linear regression with fixed slope of 1. 
The theory in these cases is easier than in the general case, but the ideas are similar. 

I hope that you will be able to better understand the general case after having worked through these two special cases.





