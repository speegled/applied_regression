# Variable Selection

We have already seen times when we have more predictors than optimal for predicting the response. We discussed an *ad hoc* method of variable selection using $p$-values, which can be useful for explanatory model building. We also looked at projecting the predictors onto a smaller subspace using PCA, which works about as well as projecting randomly onto subspaces in many cases. 

The issue with PCA regression is that the choice of directions of maximal variance may have nothing to do with the response. It **could* be that the predictive value of the predictors is in a direction that has small variance relative to other directions, in which case it would be hidden in one of the later components. For this reason, we will want to consider the relationship of the components in the predictors with the response. That is the idea behind partial least squares.

In the next section, we will consider ridge and lasso regression. These methods attempt to minimize an error function that also includes the magnitudes of the coefficients. The thought is that if a predictor doesn't have much predictive power, then the penalty associated with the coefficient will force the coefficient to be zero, which is the same as eliminating it from the model.

Next, we will talk about the Akaike Information Criterion, which is a classical way of variable selection that also performs well in predictive models.

Finally, we will take a **long** overdue detour and talk about interactions. Interactions really belonged in the classical theory section, but are also useful for predictive modeling. We will discuss their use both in explanatory model building and predictive model building.

## Partial Least Squares

As mentioned above, partial least squares first finds the direction in the predictor space that has maximum covariance with the response. Let's do that using simulations so that we can understand what is going on. Our data is also simulated data.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(caret)
```


```{r}
set.seed(2162020)
Sigma <- matrix(rnorm(9), nrow = 3)
Sigma <- Sigma %*% t(Sigma)
dd <- as.data.frame(MASS::mvrnorm(500, mu = c(0,0,0), Sigma = Sigma))
names(dd) <- c("x", "y", "z")
cor(dd)
dd <- mutate(dd, response = 2 * (x + 4 *y) + .25 * (x + 6*y + z) + rnorm(500, 0, 7))
summary(lm(response ~., data = dd))
```

We begin by centering and scaling the predictors.

```{r}
dd <- dd %>% mutate(x = scale(x),
             y = scale(y),
             z = scale(z))
```

Next, as in the case of PCA, we will use `optim` to maximize the covariance of the projection of the data set onto a direction on the unit sphere in $R^3$. We begin with some helper functions, which are the inner product of two vectors and the norm of a vector.

```{r}
ip <- function(x, y) {
  sum(x * y)
}
mynorm <- function(x) {
  sqrt(sum(x^2))
}
```

Next, we see how to take a random sample from the sphere, project the data onto that direction, and compute the covariance with the response.

```{r}
rvec <- rnorm(3)
rvec <- rvec/mynorm(rvec)
dd_p <- apply(dd[,1:3], 1, function(x) ip(x, rvec))
cov(dd_p, dd$response)
```

Now we put this inside of `optim`. We normalize the vector and take the negative absolute value of the covariance in order to insure that the *minimum* found corresponds to the *maximum covariance*.

```{r}
ff <- function(rvec) {
  rvec <- rvec/mynorm(rvec)
  dd_p <- apply(dd[,1:3], 1, function(x) ip(x, rvec))
  -abs(cov(dd_p, dd$response))
}
first_comp <- optim(par = c(-.33,-.66, .66), fn = ff, control = list(reltol = .000001))
first_comp
first_comp$par/mynorm(first_comp$par)
```

According to this, our estimate for the first PLS component is `r round(first_comp$par/mynorm(first_comp$par), 3)`. This is the direction of the data that has the largest covariance with the response. Let's check it against the first component found using the `plsr` function in the `pls` package.

```{r}
library(pls)
plsmod <- plsr(response ~ x + y + z, data = dd)
loadings(plsmod)
plsmod$projection
```

We see that the first component that `plsr` got in the `projection` is $(0.331, 0.668, -0.6667)$, which is the negative of what we got, so gives the same line or direction. 

Now, in PCA to get the second component, we looked at all directions perpendicular to the first component and found the one with maximum variance. In PLS, we no longer force the second component to be orthogonal to the first component. That's one difference. But, we do want it to pick up different information than the first component. How do we accomplish that?

A first idea might be to find the direction that has the highest covariance with the **residuals** of the response after predicting on the directions chosen so far. That is pretty close to what happens, but doesn't work exactly. In partial least squares, we first **deflate** the predictor matrix by subtracting out $d d^T M$, where $d$ is the first direction, $d^T$ is the transpose of $d$, and $M$ is the original matrix. Then, we find the direction of maximum covariance between the residuals of the first model and this new matrix. I include the details below, but this section is **OPTIONAL**.

### Optional PLS Stuff

```{r}
#first_vec <- largest_direction
dd$direction_1 <- apply(dd[,1:3], 1, function(x) ip(x, first_vec))
m1 <- lm(response ~ direction_1, data = dd)

pvec <- c(1,0,0)
#first_vec
#dd$direction_1 <- apply(dd[,1:3], 1, function(x) ip(x, first_vec))
E <- as.matrix(dd[,1:3])
new_dat <- E - dd$direction_1%*% t(dd$direction_1) %*% E /(mynorm(dd$direction_1)^2)
projections <- apply(new_dat, 1, function(x) ip(x, pvec))
best_cov <- abs(cov(m1$residuals, projections))
best_dir <- pvec
for(i in 1:1000) {
  pvec <- best_dir + rnorm(3,0,.02)
  pvec <- pvec/mynorm(pvec)
  projections <- apply(new_dat, 1, function(x) ip(x, pvec))
  covv <- abs(cov(m1$residuals, projections))
  if(covv > best_cov) {
    best_cov <- covv
    best_dir <- pvec
    print(pvec)
  }
}
best_dir
plsmod$loading.weights
```

As we can see, we have recovered the *loading weights*, which map the **deflated** predictors onto the scores. Continuing in this fashion, we could find all of the loading weights, use them to compute the scores, and then compute the loadings via

```{r}
corpcor::pseudoinverse(as.matrix(dd[,1:3])) %*% plsmod$scores
```

And this would give us our projection, which we could use to transform the original (or new) data into the basis spanned by the partial least squares representation. The trickiest part about this whole thing is understanding why we deflate the data each time we get a new component in our partial least squares. Let's leave that for another time, shall we?

```{r}
as.matrix(dd[,1:3]) %*% plsmod$projection %>% 
  head()
plsmod$scores[1:6,1:3]
```

## Ridge/Lasso and Elastic Net

PCR and partial least squares regression both have the goal of reducing the number of predictor variables by replacing the given predictors with a smaller set of linear combinations of the predictors. This has the advantage of finding directions in the data that are aligned with either the signal of the data or the response, but has as a disadvantage that it can be hard to interpret the new predictors. The techniques of this section work directly with the predictors, and lead to models that tend to be more easily interpretable. 

### Ridge Regression

We start with **ridge regression**. The basic idea is that minimizing SSE has no penalty at all on having arbitrarily large coefficients. If we add a penalty related to the size of the coefficients, then minimizing this new cost function will keep the coefficients relatively small. Ideally, it would force some of them to be either zero, or close enough to zero that we could round to zero and remove that variable from the model that we are building. 

Some more details. Suppose our model is

\[
y = \beta_0 + \sum_{p = 1}^P \beta_p x_p + \epsilon
\]

Recall that the SSE based on data is
\[
SSE = \sum_{i = 1}^n\bigl(y_i - \hat y_i\bigr)^2
\]
where $\hat y_i$ is the estimate of the response $y$. In ridge regression, we add a penalty based on the size of the coefficients $\beta_p$:

\[
SSE_{L^2} = \sum_{i = 1}^n \bigl(y_i - \hat y_i\bigr)^2 + \lambda \sum_{p = 1}^P \beta_p^2
\]
Note that there is also a tuning parameter $\lambda$ in the SSE. Depending on the scales of the predictors and the responses, as well as the relationship between the response and predictor, different $\lambda$ will need to be chosen.

Let's see how this works with a simple simulated example. It is suggested to center and scale our predictors before using ridge regression. We begin with an OLS model. 

```{r}
sigma <- matrix(rnorm(9), nrow = 3)
sigma <- t(sigma) %*% sigma
dd <- MASS::mvrnorm(500, mu = c(0,0,0), Sigma = sigma)
response <- dd[,1] + 2 * dd[,2] + rnorm(500,0,3)
dd <- scale(dd)
lm(response ~ dd) %>% summary()
```

Now, let's choose $\lambda = 1$ and minimize the $SSE_{L^2}$ cost function.

```{r}
ssel2 <- function(dd, response, beta_0, beta, lambda = 1) {
  sum((dd %*% beta + beta_0 - response)^2) + lambda * sum(beta^2)
}
ssel2(dd, response, 0, c(0,0,-1))
```

Now, if we want to minimize this, we use the `optim` function. First, we define the cost function just in terms of the variables that we are trying to minimize.

```{r}
sse_for_optim <- function(beta) {
  ssel2(dd, response, beta[1], beta[2:4])
}
optim(c(0,0,0,0), fn = sse_for_optim)
lm(response ~ dd)
```

We see that regularizing with $\lambda = 1$ had little impact on the result! Since the actual minimum SSE is about 4000, and the coefficients are about 1, that makes sense. In order to get them on the same scale, we would need to have $\lambda = 1000$ or so. Let's try again with $\lambda = 1000$ just to see. Before we do this, you should think about whether larger $\lambda$ will be associated with coefficients that are closer to what we get with OLS, or further.

```{r}
sse_for_optim <- function(beta) {
  ssel2(dd, response, beta[1], beta[2:4], lambda = 1000)
}
optim(c(0,0,0,0), fn = sse_for_optim)
lm(response ~ dd)
```

Still not a huge difference, but note that the large coefficients are now closer to the same size. If we do ridge regression with highly correlated predictors, it is often the case that the coefficients associated with the highly correlated predictors end up being about the same size. That is, ridge regression tends to spread out the weight of that direction across all of the predictors. 

There are several R packages that implement ridge regression. One that gives the same answer as the algorithm described in the book is `ridge`.

```{r}
ridge::linearRidge(response ~ dd, lambda = 1000, scaling = "scale")
```

However, as we saw above, it can be tricky to find the right $\lambda$, so other packages do more scaling and trickery, which means that their answers have twists from the algorithm that we described above. Here is the technique suggested by the book.

```{r}
emod <- elasticnet::enet(x = dd, y = response, lambda = 1000)
enetCoef<- predict(emod, newx = dd,
                   s = 1, 
                   mode = "fraction",
                   type = "coefficients")
enetCoef
```

As long as the method is internally consistent with how it penalizes, it doesn't matter so much whether it matches the exact definition we gave; it's more of a frustration than a problem. As before, in any real problem, we will use cross validation to decide which $\lambda$ to pick. We could again use `train`, as below:


```{r}
library(caret)
data("tecator")
train(x = as.data.frame(absorp),
      y = endpoints[,1],
      method = "ridge")
```

But we can get some better information more easily out of the dedicated package `glmnet`. Let's see how to do it. I am following [this book](https://daviddalpiaz.github.io/r4sl/regularization.html) which has tons of interesting things!

```{r}
library(glmnet)
fit_ridge <- glmnet(x = absorp, y = endpoints[,1], alpha = 0)
plot(fit_ridge, label = T, xvar = "lambda")
```

Here, we can see that as $\lambda$ gets bigger the coefficients of the variables are forced to be smaller and smaller, until eventually they are essentially zero.

```{r}
fit_ridge_cv <- cv.glmnet(absorp, endpoints[,1], alpha = 0)
plot(fit_ridge_cv)
```

The first vertical dotted line is where the smallest MSE is, and the second line is where the MSE is one standard error away from the smallest. In this case, **larger** $\lambda$ corresponds to **simpler** models, or at least, models that have been more regularized. 

The function `glmnet` returns the models associated with each of those two $\lambda$'s for further investigation and use. We can see the coefficients used in the model:

```{r}
plot(coef(fit_ridge_cv, s = "lambda.min"), type = "l")
points(coef(fit_ridge_cv, s = "lambda.1se"), type = "l", col = 2)
```

We see that there isn't a big difference in the coefficients for the two models! If we wanted to predict values, we could use `predict`.

```{r}
predict(fit_ridge_cv, newx = absorp[1:2,], s = "lambda.min")
predict(fit_ridge_cv, newx = absorp[1:2,], s = "lambda.1se")
```

```{r}
plot(predict(fit_ridge_cv, newx = absorp, s = "lambda.min") - predict(fit_ridge_cv, newx = absorp), ylab = "Moisture", main = "Difference in Predictions of Moisture Content")
```

Ridge regression is a regularization technique. The point is that if there are highly correlated variables and we resample, we are likely to get similar results using ridge regression with a similar penalty. Let's check that out. We will need to come up with a covariance matrix that produces highly correlated predictors.

```{r}
sigma <- matrix(runif(16, .9, 1.1), nrow = 4)
sigma <- sigma %*% t(sigma)
df <- MASS::mvrnorm(500, mu = c(0,0,0,0), Sigma = sigma)
cor(df)
response <- df[,1] + 2 * df[,2] + 3 * df[,3] + 4 * df[,4] + rnorm(500, 0, 3)
lm(response ~ df)
```

You should check that if you run the above code multiple times, you get quite different answers for the regression coefficients. Now, let's do ridge regression.

```{r}
ridge_model <- cv.glmnet(x = df, y = response, alpha = 0)
coefficients(ridge_model)
```

The ridge-regression model typically gives a model of about $t = 2.5 x_1 + 2.5 x_2 + 2.5 x_3 + 2.5 x_4$, which is about $y = 10 x_i$ for any of the predictors $x_i$. 

### LASSO

Unlike ridge regression, the LASSO is more of a variable selection technique. If we are trying to force the non-important or redundant predictors' coefficients to be zero, we would want to use the so-called $L_0$ norm as a penalty:

\[
SSE_{L_0} = \sum\bigl(\hat y_i - y_i\bigr)^2 + \lambda\bigl| \{p: \beta_p \not= 0\}\bigr|
\]

However, this is a hard problem to solve when there are lots of variables! It essentially involves computing the SSE for each subset of predictors and seeing which one is the smallest. If we had 100 predictors, like in `tecator`, that would mean $2^{100} \approx 1,267,561,000,000,000,000,000,000,000,000$ subsets. That's gonna take some time, and even if we could do it, what if we found a data set that had 1000 predictors? I don't have that kind of time. Instead, we solve the **convex relaxation** of the problem, which is to minimize

\[
SSE_{L_1} = \sum\bigl(\hat y_i - y_i\bigr)^2 + \lambda \sum_{i = 1}^P |\beta_i|
\]

Again, we don't typically penalize the intercept, though we could if that made sense for the problem you were working on. Let's see how it works when we have the same four highly correlated synthetic data as we had in the last section.

```{r}
sigma <- matrix(runif(16, .9, 1.1), nrow = 4)
sigma <- sigma %*% t(sigma)
df <- MASS::mvrnorm(500, mu = c(0,0,0,0), Sigma = sigma)
response <- df[,1] + 2 * df[,2] + 3 * df[,3] + 4 * df[,4] + rnorm(500, 0, 3)
```

We will again use `glmnet`.

```{r}
mod <- glmnet(x = df, y = response, alpha = 1)
plot(mod, xvar = "lambda", label = TRUE)
```

```{r}
cv_mod <- cv.glmnet(x = df, y = response, alpha = 1)
plot(cv_mod)
```

```{r}
coef(cv_mod)
```

We see that the LASSO removed two of the variables, and the other two add up to about 10. (At least, that is what it seemed to be doing when I was practicing!) If we resample, then the LASSO will likely select different variables with different coefficients. So, LASSO doesn't regularize as much as ridge regression, but it does do some variable selection. Let's run it again on `tecator`.

```{r}
mod <- glmnet(x = absorp, y = endpoints[,1], alpha = 1)
plot(mod, xvar = "lambda", label = TRUE)
```

```{r}
cv_mod <- cv.glmnet(x = absorp, y = endpoints[,1], alpha = 1, type.measure = "mse")
plot(cv_mod)
```

```{r}
plot(coef(cv_mod))
sum(abs(coef(cv_mod)) > 0)
```

This model has `r sum(abs(coef(cv_mod)) > 0)` non-zero variables. That is quite a few more than PCR and PLS ended up with, but **these are the original variables** so we might have some hope of interpreting things. So, downside is more variables, upside is that the variables are more immediately interpretable. Now let's just see what the estimated RMSE was. The first one is for the $\lambda$ that is one standard error away from the minimum value, and the second is the minimum MSE observed.


```{r}
cv_mod$cvm[cv_mod$lambda == cv_mod$lambda.1se]
cv_mod$cvm[cv_mod$lambda == cv_mod$lambda.min]
```

## Step AIC

We mentioned before that we would ideally like to minimize $SSE_{L_0}$, which is the sum-squared error plus a penalty based on how many of the coefficients are non-zero. While checking all subsets of coefficients would take too long, we can try a *stepwise* approach, which would approximate this. Let's see how this would work in the example from above that is running through the chapter, but let's make the correlation between variables smaller.

```{r}
set.seed(2292020)
sigma <- matrix(runif(16, -.5, 1.5), nrow = 4)
sigma <- sigma %*% t(sigma)
df <- MASS::mvrnorm(500, mu = c(0,0,0,0), Sigma = sigma)
response <- df[,1] + 2 * df[,2] + 3 * df[,3] + rnorm(500, 0, 3)
cor(df)
```
We do LOOCV on the full model, then we do LOOCV estimate of MSE after removing each variable separately. So, in all, we do 5 LOOCV MSE estimates. If none of the 4 reduced models are better, then we are done. If one is better, that becomes our new "full model" and we see whether we can remove any of the other three variables. 

One twist, though, is that in the real algorithm, we would also want to check whether adding variables back in would improve the MSE. We will not implement that here.

The reason that we are using LOOCV is that it is easy to compute for linear models without actually doing the interation! We won't go through the details, but we will include the code here for anyone interested.

First, we find the hat matrix and the leverage values.

```{r}
dd <- as.data.frame(df)
dd$response <- response
full_mod <- lm(response ~ ., data = dd)

df_intercept <- cbind(df, rep(1, 500))
hat_matrix <- df_intercept %*% 
  solve(t(df_intercept) %*% 
          df_intercept) %*% 
  t(df_intercept)  #this is the hat matrix

diag(hat_matrix)[1:5] 
hatvalues(full_mod)[1:5] #see, the diagonal of the hat matrix is given by hatvalues!
predict(full_mod)[1:5]
(hat_matrix %*% response)[1:5] 

```

The hat matrix is called the hat matrix because when you multiply the hat matrix times the response, you get the predicted values! Sounds like magic, I know, but you can see that it is true above.

You can also get the residuals from the hat matrix:

```{r}
((diag(1, 500) - hat_matrix) %*% response)[1:5]
full_mod$residuals[1:5]
```

So far, this doesn't seem too magical. But, check this out. We can get the LOOCV MSE score directly from the hat matrix.

```{r}
1/500 * sum(((diag(1, 500) - hat_matrix) %*% response)^2/(1 - hatvalues(full_mod))^2)
```
Perhaps easier to understand:

```{r}
mean((full_mod$residuals^2)/(1 - hatvalues(full_mod))^2)
```

It is the average value of the residuals squared, weighted by the reciprocal of 1 minus the leverage squared. Let's verify through actual LOOCV.

```{r}
errors <- sapply(1:500, function(x) {
  dd_small <- dd[-x,]
  mod_small <- lm(response ~ ., data = dd_small)
  predict(mod_small, newdata = dd[x,]) - dd$response[x]
})
mean(errors^2) #behold!
```

Now, let's remove each of the four variables separately, and estimate the MSE via LOOCV.

```{r}
loocvs <- sapply(1:4, function(x) {
  dd_skinny <- dd[,-x]
  skinny_mod <- lm(response ~ ., data = dd_skinny)
  mean((skinny_mod$residuals^2)/((1 - hatvalues(skinny_mod))^2))
})
loocvs
```
By inspection, we see that we should remove variable number `r which.min(loocvs)`, because doing so has the lowest LOOCV estimate of MSE, and it is lower than the full model, which was `r mean(errors^2)`. Then, we would repeat the above, seeing if we can remove any of variables 1, 2 and 3. 

There is another way of doing this, using the Akaike Information Criterion. It can be shown that under some assumptions, the AIC is asymptotically equivalent to doing LOOCV. Here, by asymptotically equivalent, I mean that as the number of data points goes to infinity, LOOCV with MSE and the AIC will both yield the same suggestion as to which variable to remove, or whether to stay with the model you have.

```{r}
MASS::stepAIC(full_mod, steps = 1)
```

Note that, in this case, `stepAIC` also chooses to remove the 4th variable. It is not the case that these two techniques will always give the same suggestion, though, even though in the limit as the number of observations goes to infinity they will. 

Let's try them out on the tecator data set.

```{r}
data("tecator")
compute_loocv <- function(full_model) {
  mean((full_model$residuals)^2/((1 - hatvalues(full_model))^2))
}
dd <- as.data.frame(absorp)
dd$response <- endpoints[,1]
full_model <- lm(response ~ ., data = dd)
mse_full <- compute_loocv(full_model)
mse_loocv <- sapply(1:ncol(absorp), function(x) {
  dd_skinny <- dd[,-x]
  skinny_model <- lm(response ~ ., data = dd_skinny)
  compute_loocv(skinny_model)
})
which.min(mse_loocv)
mse_loocv[45]
```
This suggests that our first move should be to remove variable 45, resulting in a decrease of almost a full unit in MSE!  Now, let's see what `stepAIC` suggests.

```{r}
MASS::stepAIC(full_model, steps = 1)
```

Hmmm, it suggests to remove variable 53. 

```{r}
mse_loocv[53]
mse_loocv[45]
```

This really isn't doing a good job of selling itself as a proxy for LOOCV.

### Doing LGOCV with preprocessing

1. If there are tuning parameters, set tuning parameter.
1. Split into test/train
2. Preprocess train based only on train.
3. Build model.
4. Process test based on the preprocessing model from 2.
5. Predict response from model built in 3.
6. Compute error
7. Repeat 1-6 100-ish times.
8. Take mean and standard deviation; that is the mean error estimate for this tuning parameter value.
9. Repeat for next tuning parameter value.
10. Choose the tuning parameter value that gives the simplest model that is within one standard deviation of the smallest mean error. 

Then, if you want to apply your model to new data that comes in, you build your model on the entire data with the best tuning parameter, including any preprocessing steps. Apply those same preprocessing steps to the new data and then predict. 

Let's run through the entire thing with the Chemical Manufacturing Process data.

```{r}
library(AppliedPredictiveModeling)
data("ChemicalManufacturingProcess")
```

Let's pretend like the very last observation is the new data that will be coming in at the end.

```{r}
newdata <- ChemicalManufacturingProcess[176,]
ChemicalManufacturingProcess <- ChemicalManufacturingProcess[-176,]
```

We'll do Johnson-Lindenstrauss dimension reduction followed by regression. We'll choose either 2 or 3 dimensions to project onto, for simplicity. 

```{r cache=TRUE}
# 1. If there are tuning parameters, set tuning parameter.

num_dim <- 2

# 1. Split into test/train

train_indices <- sample(1:nrow(ChemicalManufacturingProcess), 135)
test_indices <- (1:175)[-train_indices]
train_data <- ChemicalManufacturingProcess[train_indices,]
test_data <- ChemicalManufacturingProcess[test_indices,]

# 2. Preprocess train based only on train. We only want to do this for the predictors.

train_predictors <- as.matrix(select(train_data, -Yield))
train_response <- train_data$Yield
pre_model <- preProcess(x = train_predictors, 
                        method = c("center", 
                                   "scale", 
                                   "medianImpute", 
                                   "nzv"))
train_predictors <- predict(pre_model, newdata = train_predictors)

# 3. Build model.

jl_matrix <- matrix(rnorm(ncol(train_predictors) * num_dim), ncol = num_dim)
projected_predictors <- train_predictors %*% jl_matrix
mod <- lm(train_response ~ ., data = as.data.frame(projected_predictors))

# 4. Process test based on the preprocessing model from 2.

test_predictors <- as.matrix(select(test_data, -"Yield"))
test_response <- test_data$Yield
test_predictors <- predict(pre_model, newdata = test_predictors)


# 5. Predict response from model built in 3.

test_projected <- test_predictors %*% jl_matrix
test_response_model <- predict(mod, newdata = as.data.frame(test_projected))

# 6. Compute error

error <- test_response_model - test_response
mean(error^2)

# 7. Repeat 1-6 100-ish times.

lgocv_errors <- replicate(100, {
  train_indices <- sample(1:nrow(ChemicalManufacturingProcess), 135)
  test_indices <- (1:175)[-train_indices]
  train_data <- ChemicalManufacturingProcess[train_indices,]
  test_data <- ChemicalManufacturingProcess[test_indices,]
  
  train_predictors <- as.matrix(select(train_data, -Yield))
  train_response <- train_data$Yield
  pre_model <- preProcess(x = train_predictors, 
                          method = c("center", 
                                     "scale", 
                                     "medianImpute", 
                                     "nzv"))
  train_predictors <- predict(pre_model, newdata = train_predictors)
  
  jl_matrix <- matrix(rnorm(ncol(train_predictors) * num_dim), ncol = num_dim)
  projected_predictors <- train_predictors %*% jl_matrix
  mod <- lm(train_response ~ ., data = as.data.frame(projected_predictors))
  
  test_predictors <- as.matrix(select(test_data, -"Yield"))
  test_response <- test_data$Yield
  test_predictors <- predict(pre_model, newdata = test_predictors)
  
  test_projected <- test_predictors %*% jl_matrix
  test_response_model <- predict(mod, newdata = as.data.frame(test_projected))
  
  error <- test_response_model - test_response
  mean(error^2)
})

# 8. Take mean and standard deviation; that is the mean error estimate for this tuning parameter value.

c(mean = mean(lgocv_errors), sdev = sd(lgocv_errors))

# 9. Repeat for next tuning parameter value.

sapply(2:3, function(num_dim) {
  lgocv_errors <- replicate(100, {
    train_indices <- sample(1:nrow(ChemicalManufacturingProcess), 135)
    test_indices <- (1:175)[-train_indices]
    train_data <- ChemicalManufacturingProcess[train_indices,]
    test_data <- ChemicalManufacturingProcess[test_indices,]
    
    train_predictors <- as.matrix(select(train_data, -Yield))
    train_response <- train_data$Yield
    pre_model <- preProcess(x = train_predictors, 
                            method = c("center", 
                                       "scale", 
                                       "medianImpute", 
                                       "nzv"))
    train_predictors <- predict(pre_model, newdata = train_predictors)
    
    jl_matrix <- matrix(rnorm(ncol(train_predictors) * num_dim), ncol = num_dim)
    projected_predictors <- train_predictors %*% jl_matrix
    mod <- lm(train_response ~ ., data = as.data.frame(projected_predictors))
    
    test_predictors <- as.matrix(select(test_data, -"Yield"))
    test_response <- test_data$Yield
    test_predictors <- predict(pre_model, newdata = test_predictors)
    
    test_projected <- test_predictors %*% jl_matrix
    test_response_model <- predict(mod, newdata = as.data.frame(test_projected))
    
    error <- test_response_model - test_response
    mean(error^2)
  })
  
  c(mean = mean(lgocv_errors), sdev = sd(lgocv_errors))
})


# 10. Choose the tuning parameter value that gives the simplest model that is within one standard deviation of the smallest mean error. 
```

By inspection, the model with `num_dim = 2` is the one that is the best.

## Interactions

This section is long, long overdue. We are going to talk about interactions between variables. It can, and often does, happen that the values of one variable affect the way that another variable affects the response. For example, suppose you were measuring how fast honey emtpied out of a jar as a function of the angle that the jar is held and the ambient temperature. It is reasonable to wonder whether at higher temperatures, the angle will have a different effect on how quickly the honey drains than at lower temperatures. Another example would be estimating the drop in blood pressure per mg of drug pressure medicine administered. It is reasonable to wonder whether the rate at which drug pressure drops is different per mg for women than for men, or for children than for adults. 

The interpretation of interactions is different depending on whether the predictors are categorical or numeric.

1. For categorical data, interactions are an effect for each combination of the levels in the categorical data.

2. For an interaction between categorical and numeric, interactions indicate a different slope for each of the levels of the categorical data.

3. For interaction between numeric data, the interactions indicate that at a fixed level of the first variable, there is a linear response in the other variable, but the slope of that response changes with the value of the first variable. In practice, interaction between numeric variables creates a new variable which is the product of the two variables, and includes that in the regression model.

Let's look at an example from ISwR.

```{r}
juul <- ISwR::juul
```

We model `igf` on age, sex and tanner level, with interactions.

```{r}
juul$sex <- factor(juul$sex, levels = c(1, 2), labels = c("boy", "girl"))
juul$tanner <- factor(juul$tanner)
mod <- lm(igf1 ~ (age + sex + tanner)^2, data = juul) #includes all interactions between pairs of variables
summary(mod)
```

Let's go through the interpretations. Note that if we do not re-code `sex` and `tanner` as categorical, then our output would have been quite different. 

Looking at the `age` coefficient, we would be tempted to say that the `igf` level is associated with an increase of 14.887 micrograms/liter for each year of age increase. However, that can (read: would) be misleading. Looking further down the list, we see coefficients associated with `age:sex` and `age:tanner`. So, the increase of 14.886 per year is **for boys** in **tanner level 1**. 

If we look at `age:sexgirl`, we see that we need to add 5.666 to the estimate for boys in order to get the estimate for the `igf` level increase per year. In other words, for girls (in tanner level 1), the increase is `14.866 + 5.666 = 20.532`. R has as its default that it reports the values for the first level of the factor, and then the interactions indicate the change from that first value for the other levels. It chose `boys` because we listed `boy` before `girl` in the `levels` argument to `factor`. It is important to know which level R is considering the base level for interpreting the coefficients!

Note that we didn't specify the levels in the `tanner` variable. In this case, R chooses a natural order for the levels. If the data is numeric, it will arrange them in increasing order, and if the data is character, then it will arrange them in alphabetical order. If we want to be sure, we can type `levels(juul$tanner)` and it will give us the labels of the levels in the order that it has them.

```{r}
levels(juul$tanner)
```

We see that tanner level 1 is the "base" level. So, the `igf` increases by 14.886 per year for **boys** in **tanner 1**. Girls in tanner 2 have a an estimated increase of 14.886 + 5.666 + 10.759 per year. (Note: this sounds like I am saying that for an individual girl, she would expect to see her igf1 level increase by this much per year. That is not what I mean. I mean that each extra year is associated with igf1 levels that are 31.291 micrograms/liter higher. What is the difference?) 

Now, let's estimate the mean `igf` level for girls in tanner 2 who are 12 years old. It would be 

\[
79.825 -32.310 - 69.726 + (14.886124 + 5.666282 + 10.758917 ) * 12 + 19.597 =  373.1219
\]

I have used more sig digits in the slope of age because we are multiplying by 12, and otherwise it doesn't match the answer given by predict very well.

```{r}
predict(mod, newdata = data.frame(age = 12,
                                  sex = "girl",
                                  tanner  = "2"))
```

Now let's look at significance. Dalgaard writes "The exact definition of the interaction terms and the interpretation of their associated regression coefficients can be elusive. Some peculiar things happen if an interaction term is present but one or more of the main effects are missing." Note that if we re-do the model with the main effect `sex` missing, it simply reparametrizes the model. The $R^2$ and $F$ statistic are exactly the same.

```{r}
mod2 <- lm(igf1 ~ age + tanner + age:tanner + sex:tanner + sex:age, data = juul)
summary(mod2)
```

Let's remind ourselves what the original model summary was.

```{r}
summary(mod)
```

We see a significant interaction between sex and tanner, between age and tanner, and but not between age and sex. It might be reasonable to remove the interaction term between age and sex. I don't normally use `update`, but now would be a good time to start. Here are two ways of specifying the model.

```{r}
mod2 <- lm(igf1 ~ age + sex + tanner + age:tanner + sex:tanner, data = juul)
summary(mod2)
mod2 <- update(mod, formula. = . ~. -age:sex)
summary(mod2)
```

At this point, the interaction between sex and tanner is significant, as is the interaction between age and tanner. I would not proceed any further with reducing the model. I tend to only consider removing main effect terms if all of the interactions associated with those terms are not significant. Note also that this is the default behavior of `stepAIC`, which can also be applied to models with interactions.

```{r}
MASS::stepAIC(mod)
```



