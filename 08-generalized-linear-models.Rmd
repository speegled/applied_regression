# Generalized Linear Models

In this chapter, we discuss a family of models called *generalized linear models*. These models include ordinary least squares regression, and many others. All of the models presented in this chapter can be realized as examples of a common framework. We won't present the common framework in this book, but focus on two specific examples - *logistic regression* and *Poisson regression*. We will also return to the more classical framework of regression, where we are interested in the theory and assessing the fit of the model, rather than focussing exclusively on predictive modeling. 

Since generalized linear models include OLS, let's take a brief review of OLS. Recall that our model is

\[
y = \beta_0 + \sum_{i = 1}^p \beta_i x_i + \epsilon
\]

where $\epsilon$ is normal with mean 0 and unknown variance $\sigma^2$. Another way of thinking about this is that we are modeling the *expected value* of the response via a linear (affine) equation.

\[
E[y] = \beta_0 + \sum_{i = 1}^p \beta_i x_i
\]

and we are assuming that we have *normal errors* or deviations from the expected value. Another way to think about it is that the response $y$ is normal with mean $\mu = \beta_0 + \sum_{i = 1}^p \beta_i x_i$ and variance $\sigma^2$.

## Logistic Regression

Logistic regression is used to model responses that can only be two values. Such variables are called *binary* variables. Examples include determining whether a flipped coin comes up as Heads or Tails, or whether a person recovers or doesn't recover from a disease, or whether a person votes yes or no in an election. Many times, we have additional information that is naturally grouped with the outcome. For example, we may have a persons age, height, weight, the amount and type of medicine they were given in addition to whether they recovered or didn't recover from the disease. The goal is to be able to model the recovery or non-recovery from the disease on the predictors. 

We'll start in the simple case where we have a single predictor and a binary response. Let's have a specific data set in mind so that we can be concrete about things. 

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
set.seed(4072020)
df <- data.frame(x = rnorm(200, 0, 1.2))
df <- mutate(df, response = rbinom(200, 1, 1/(exp(-1 - 2*x) + 1)))
```

Let's look at some summary statistics.

```{r}
summary(df)
```

```{r}
plot(df$x, df$response)
```

This is a very typical and promising plot for wanting to do logistic regression. We see that as $x$ gets larger, more of the responses are 1 and as $x$ gets smaller, more of the responses are 0. We will see later that this synthetic data was generated **exactly** according to the logistic regression model, much like a response of `1 + 2*x + rnorm(100, 0, 3)` would be exactly according to OLS. 

If we are reasoning analogously to OLS, we might think that we should try to model the *expected value* of the response as a linear (affine) equation of the predictors, as in

\[
E[y] = \beta_0 + \sum_{i = 1}^p \beta_i x_i.
\]

Sometimes that is appropriate, but if we are thinking of the response as a 0/1 random variable, then its expected value is the **probability** of a success. The affine equation above can be negative or bigger than 1, which would be impossible for a probability. Alternatively, let $p = E[y]$, and we could model the **log odds** of the response as an affine equation of the predictors:

\[
\log \frac{p}{1-p} = \beta_0 + \sum_{i = 1}^p \beta_i x_i
\]

This is the approach that is taken in logistic regression. For completeness, let's solve the above equation for $p$:

\[
p = \frac{e^{\beta_0 + \sum_{i = 1}^p \beta_i x_i}}{1 + e^{\beta_0 + \sum_{i = 1}^p \beta_i x_i}} = \frac{1}{1 + e^{-\beta_0 - \sum_{i = 1}^p \beta_i x_i}}
\]

The *error* component in this model is not as easy to see as in the normal case, because it isn't simply added as a random variable. But we can see that the random component of the response is binomial with $n = 1$ and $p = \frac{1}{1 + e^{-\beta_0 - \sum_{i = 1}^p \beta_i x_i}}$. This is is often referred to as "binomial error" or "binomial noise", but the term *random component* makes more sense to me. The predictors can be either continuous or categorical, but we will be going through the derivations only for a single continuous predictor (just like we did with OLS). One final piece of terminology is the *link function*. In the case of logistic regression, the link function is $\log \frac{x}{1-x}$. This is the link between the linear combination of predictors and the expected value of the response.

For the specific example that we are considering, we have only a single predictor $x$, so our model is

\[
\log \frac{p}{1-p} = \beta_0 + \beta_1 x
\]

The first question you might have is: how do we get estimates of $\beta_i$? The classical way is using *maximum likelihood estimation*, which we will get to momentarily. But, if we are coming to this through the lense of what we have covered so far in these notes, which we are, then why not minimize some error function? Let's imagine that we have our values for $\log \frac {p}{1-p}$ in terms of $\beta_0$ and $\beta_1$, so $\log \frac {p_i}{1-p_i} = \beta_0 + \beta_1 x_{1i}$. Ideally, the log odds would be positive and very large when $y_i$ are 1, and would be negative with large absolute value when $y_i$ are 0. Here are three  natural loss functions to minimize, which were taken from [Zhang](https://projecteuclid.org/download/pdf_1/euclid.aos/1079120130). 

**Note** To proceed, we need to transform the response into a $-1/1$ random variable. That is, we change all of the zero responses to $-1$. This just simplifies the notation. We also write $\text{logit}(p_i) = \log \frac {p_i}{1 - p_i}$

The three loss functions are all of the form
\[
\frac{1}n \sum_{i = 1}^n \Phi(\text{logit}(p_i) y_i)
\]

where $\Phi$ is one of

1. $\Phi_1(x) = \max(1-x,0)^2$, *modified least squares*.
2. $\Phi_2(x) = e^{-x}$, *AdaBoost*.
3. $\Phi_3(x) = \log(1 + e^{-x})$, *logistic regression*.

In order to be a good loss function for this model, we would like for *positive values* of $\text{logit}(p_i)y_i$ to be associated with *small* errors. Think about this: if the log odds are bigger than 0, then $p_i > 1 - p_i$, so $p_i > 1/2$. So, if the log odds are bigger than zero and the response is 1, that's a good thing! Similarly if the log odds are less than zero and the response is $-1$, then that's also a good thing! The bigger the value of $\text{logit}(p_i)y_i$, the better it predicts. The smaller the value of $\text{logit}(p_i)y_i$ (i.e., big and negative), the worse it predicts [^1].

We plot the three loss functions so that you can see what the graphs look like.

```{r}
f <- function(x) {
  pmax(1 - x, 0)^2
}
curve(f, from = -3, to = 2, main = "Comparison of three loss functions") #black
curve(exp(-x), add = T, col = 2) #red
curve(log(1 + exp(-x)), add = T, col = 3) #green
```

All three functions have varying degrees of penalties for misclassifying the response. *AdaBoost* has an exponential penalty, *modified least squares* has a quadratic penalty, and *logistic regression* has a linear penalty. As usual, this has implications in terms of robustness to outliers and other aspects of the estimates.

OK, let's see how our good friend `optim` can do with these functions! We'll start with the modified least squares, even though that one definitely feels like the hardest because of a lack of differentiability at 1. We first have to get the sse function set up properly.

```{r}
sse_1 <- function(beta, dat) {
  beta_0 <- beta[1]
  beta_1 <- beta[2]
  mean(f((beta_0 + beta_1*dat$x) * dat$y))
}
names(df) <- c("x", "y")
df <- mutate(df, y = ifelse(y == 0, -1, y))
sse_1(c(0,1), df)
```

OK, here we go.

```{r}
best <- optim(par = c(0,0), fn = sse_1, dat = df)
best
```

We get values of the parameters to be $\beta_0$ =`r round(best$par[1], 3)` and $\beta_1$ = `r round(best$par[2], 3)`. Note that these values are not particularly close to the true values that we know: $\beta_0 = 1$ and $\beta_1 = 2$.  

Now, we plot the logistic probability curve on the same graph as the data. We will want to change the responses back to 0/1.

```{r}
df_zero <- df
df_zero$y <- ifelse(df$y == -1, 0, df$y)
logit <- function(p) {
  log(p/(1-p))
}
inv_logit <- function(y, beta_0, beta_1) {
  1/(1 + exp(-beta_0 - beta_1 * y))
}
beta_0 <- best$par[1]
beta_1 <- best$par[2]
plot(df_zero$x, df_zero$y)
curve(inv_logit(x, beta_0, beta_1), add = T)
```

As an exercise, you are asked to repeat the above for the other two loss functions. Before doing so, you should think a bit about what you expect to get, relative to what we got for this one. The graph you are looking to get is below.

```{r echo=FALSE}
sse_2 <- function(beta, dat) {
  beta_0 <- beta[1]
  beta_1 <- beta[2]
  mean(exp(-1*(beta_0 + beta_1*dat$x) * dat$y))
}
best_2 <- optim(par = c(0,0), fn = sse_2, dat = df)
f_3 <- function(x) {
  log(1 + exp(-x))
}
sse_3 <- function(beta, dat) {
  beta_0 <- beta[1]
  beta_1 <- beta[2]
  mean(f_3((beta_0 + beta_1*dat$x) * dat$y))
}
best_3 <- optim(par = c(0,0), fn = sse_3, dat = df)
```

```{r, echo=FALSE}
plot(df_zero$x, df_zero$y, main = "Red is sse, Green AdaBoost, Blue logistic")
beta_0 <- best$par[1]
beta_1 <- best$par[2]
curve(inv_logit(x, beta_0, beta_1), add = T, col = 2)
beta_0 <- best_2$par[1]
beta_1 <- best_2$par[2]
curve(inv_logit(x, beta_0, beta_1), add = T, col = 3)
beta_0 <- best_3$par[1]
beta_1 <- best_3$par[2]
curve(inv_logit(x, beta_0, beta_1), add = T, col = 4)
```

### Maximum Likelihood Estimation

The more traditional way of arriving at the estimates for $\beta_0$ and $\beta_1$ in logistic regression is via **maximum likelihood**. Here, if we have $p_i$ is the probability of success associated with observation $i$, the likelihood of observing $y_i$ would be $p_i^{y_i}(1 - p_i)^{1 - y_i}$. Note that this is a fancy way of writing that the likelihood would be $p_i$ if $y_i = 1$ and it would be $(1 - p_i)$ if $y_i = 0$. The likelihood of observing the sequence $y_1, \ldots, y_n$ would be given by the *product* of the individual likelihoods, since we are assuming independence. Therefore, given $p_i$, the likelihood function is

\[
\Pi_{i = 1}^n p_i^{y_i}(1 - p_i)^{1 - y_i}
\]

A common trick when dealing with likelihood functions is to take the log. We will use below the fact that a positive function $f$ has a maximum at $x = x_0$ if and only if the function $\log f$ has a maximum at $x = x_0$. Taking logs of the likelihood function, we get (after a bit of algebra)

\[
\sum_{i = 1}^n y_i \log(p_i) + (1 - y_i) \log(1 - p_i) = \sum_{\{i: y_i = 1\}} \log(p_i) + \sum_{\{i:y_i = 0\}} \log(1 - p_i)
\]

Finally, we are assuming the following relationship between $p_i$, the $\beta$'s and $x_i$: $p_i = \frac{1}{1 + e^{-\beta_0 - \beta_1 x_i}}$. Therefore, we have the following likelihood that we are trying to maximize:

\[
\sum_{\{i: y_i = 1\}} \log\biggl(\frac{1}{1 + e^{-\beta_0 - \beta_1 x_i}}\biggr) + \sum_{\{i:y_i = 0\}} \log\biggl(1 - \frac{1}{1 + e^{-\beta_0 - \beta_1 x_i}}\biggr)
\]

The sum on the right can be simplified some, but we do not bother. So, that is the function that we want to *maximize* over all choices of $\beta_0$ and $\beta_1$. Let's do it.

```{r}
log_mle <- function(beta, dat) {
  beta_0 <- beta[1]
  beta_1 <- beta[2]
  p <- inv_logit(dat$x, beta_0, beta_1)
  -1 * sum(dat$y * log(p) + (1 - dat$y) * log(1 - p))
}
optim(par = c(0, 0), fn = log_mle, dat = df_zero)
```

We see that the values for $\beta_0$ and $\beta_1$ that yield the maximum likelihood of the data are approximately the same as the values that we computed above when doing the logistic regression error function.

As an aside, we can also do maximum likelihood estimation in classical OLS. Let's see how it would work for the case of a single numeric predictor. Our model is that 

\[
y_i \sim \text{rnorm}(\beta_0 + \beta_1 x_i, \sigma)
\]

So, the basic model has three parameters in it that we will need to estimate. The likelihood of data $(x_1, y_1), \ldots, (x_n, y_n)$ given parameter values $\beta_0, \beta_1$ and $\sigma$ is given by

\[
L(\text{data}|\text{parameters}) = \Pi_{i = 1}^n \text{dnorm}(y_i, \beta_0 + \beta_1 x_i, \sigma)
\]

Given data, we would maximize the log likelihood function using `optim`.

```{r}
set.seed(4112020)
ols <- data.frame(x = runif(20, 0, 10))
ols$y <- 1 + 2 * ols$x + rnorm(20, 0, 3)

log_likelihood <- function(params, data) {
  beta_0 <- params[1]
  beta_1 <- params[2]
  sigma <- params[3]
  -1 * sum(log(dnorm(data$y, beta_0 + beta_1 * data$x, sigma)))
}

optim(par = c(0, 0, 1), fn = log_likelihood, data = ols)
summary(lm(y ~ x, data = ols))
```

We see that the maximum likelihood estimators of $\beta_0$ and $\beta_1$ are close to the values from `lm`, but the maximum likelihood estimator of $\sigma$ is different from the residual standard error reported in `lm`. If you run this code multiple times (without setting seed), you will see that this is consistently the case.

Finally, we see how to use the R function `glm` to find the parameters associated with the maximum likelihood/logistic regression error function.

```{r}
glm(y ~ x, data = df_zero, family = "binomial")
```




### Model Diagnostics

We start by looking at the (residual) deviance. This is twice the difference between the log-likelihood of the *saturated model* and the log-likelihood of the model under consideration. The saturated model has a parameter for each observation, so the estimates will be $\hat p_j = 1$ if $y_j = 1$ and $\hat p_j = 0$ if $y_j = 0$. The likelihood of this data under this set of $\hat p_j$ is exactly 1, so the log-likelihood is zero. Sjoe![^2] That's a long way of saying that the saturated model contributes nothing to the deviance, and we compute the deviance by negative 2 times the log likelihood function of our model, which was computed above.

\[
-2 \sum_j y_j \log \hat p_j + (1 - y_j) \log (1 - \hat p_j)
\]

Generally speaking, the smaller the deviance is, the closer the model under consideration is to the saturated model (which models the data perfectly). Therefore, smaller deviances are preferred to larger ones. Suppose we had another variable in our data set that wasn't related to the response it all. We would expect the deviance of a model with that predictor to be much higher than the deviance with the predictor that we know models the data pretty well, based on how we generated the data.

```{r}
df_zero$bad <- rnorm(200)
glm(y ~ bad, data = df_zero, family = "binomial")
glm(y ~ x, data = df_zero, family = "binomial")
```

Looking under the residual deviance, we see that the model with the variable `bad` is indeed bad, and has residual deviance very close to the null deviance! The null deviance is just the log-likelihood of the data under the model with a single parameter; that is, all $y_j$ are predicted by the same probability $\hat p_j = \hat p$. See that the null and residual deviance are identical in the following model.

```{r}
glm(y ~ 1, data = df_zero, family = "binomial")
```

We could use the residual deviances of nested models to test whether the variables are necessary for the model, much like we did with ANOVA in regression, but we leave that topic for another book.

In OLS, we examined the residuals in order to get a feel for whether the model's hypotheses are met. In this section, we do a similar thing for logistic regression. We do this for the case of a single predictor, but the ideas are the same for multiple predictors and/or interactions. There are some technicalities that arise in the case of categorical predictors, so we will be assuming a single, numeric predictor, as in our example. The type of residual that we will be dealing with in this section is the *deviance residual* (you don't get confused between breakfast and a fast break, do you? then why should deviance residuals and residual deviances be confusing?). 

\[
d(y_j, \hat p_j) = \begin{cases}-\sqrt{2 |\log(1 - \hat p_j)|} & y_j = 0\\
\sqrt{2 |\log (\hat p_j)|} & y_j = 1
\end{cases}
\]

We will use the deviance residuals to assess the model fit via a **Hosmer Lemeshow** test, which we now describe. 

1. Arrange the data in order based on the fitted values
2. Group the data into 10 groups of roughly equal size.
3. Compute the expected number of successes in each group, together with the observed number of successes in each group.
4. Perform a $\chi^2$ goodness of fit test. The null hypothesis is that the probabilities computed from the model are correct, and the alternative is that they are not. If we reject the null hypothesis, that means that the logistic regression model that we built is not modelling the data well. Note: this is different than saying that it performs poorly on some cross-validation metric. It is saying that the probabilities that we are assigning do not match reality, even for the data that we used to assign those probabilities!

Let's see how to do this step-by-step using R.

```{r}
# Arrange the data by fitted values

df_zero$bad <- NULL #remove the bad variable
mod <- glm(y ~ x, 
           data = df_zero,
           family = "binomial")
df_zero$fitted_values <- fitted.values(mod)
df_zero$deviance_residuals <- residuals(mod)
df_zero$pearson_residuals <- residuals(mod, type = "pearson")
df_zero <- arrange(df_zero, fitted_values)
mod <- glm(y ~ x, 
           data = df_zero,
           family = "binomial") #We recompute this so that the model matches the current data frame!


# Split into 10 groups of equal size (by fitted value)

df_zero$group <- rep(1:10, each = 20) 
df_zero %>% 
  group_by(group) %>% 
  summarize(count = n(),
            p = mean(fitted_values))

# Compute the expected and observed number of successes

df_zero %>% 
  group_by(group) %>% 
  summarize(expected = sum(fitted_values),
            observed = sum(y))


```

Now, for those of you who have used $\chi^2$ tests a lot, you see that we have a problem. The normal rule of thumb is to require the expected number of each cell to be at least 5 or so in order for the test to be run. For this reason, we should probably have used fewer groups, so that the expected values wouldn't be so close to zero. 

Another issue with what we have done so far is that we have arbitrarily only counted the expected successes. We can also count the expected failures and bind it to the bottom.

```{r}
# Expected and observed number of failures

df_zero %>% 
  group_by(group) %>% 
  summarize(expected = sum(1 - fitted_values),
            observed = sum(1 - y))


for_chisq <- rbind(
  df_zero %>% 
  group_by(group) %>% 
  summarize(expected = sum(fitted_values),
            observed = sum(y)),
  df_zero %>% 
  group_by(group) %>% 
  summarize(expected = sum(1 - fitted_values),
            observed = sum(1 - y)))
for_chisq
```

Now, we perform the $\chi^2$ goodness of fit test on the expected and the observed cell levels. According to Hosmer and Lemeshow, the value of `t_stat` given below should have approximately a $\chi^2$ distribution with 8 degrees of freedom; that is, the number of groups minus 2.

```{r}
t_stat <- sum((for_chisq$observed - for_chisq$expected)^2/for_chisq$expected)
pchisq(t_stat, 8, lower.tail = F)
library(ResourceSelection)
hoslem.test(x = df_zero$y, df_zero$fitted_values, g = 10)
```

This $p$-value is not likely to be accurate, since we know that the assumptions of the $\chi^2$ goodness of fit test have not been met. Let's re-run it with $g = 8$, which will likely be closer to meeting the assumptions, though $g = 10$ is certainly the traditional choice.

```{r}
hoslem.test(x = df_zero$y, df_zero$fitted_values, g = 8)
```

We can see some problems with this test of fit. First, we are very likely to have expected cell counts that are close to zero, depending on the data set. This problem will be mitigated somewhat as the number of observations gets larger. A second problem is that we can get different recommendations based on the number of cells that we use. A third problem is that we are not sure what the power of this test is, even if it does meet all of the assumptions. The Hosmer-Lemeshow test for fit is widely used, but for the above reasons, it is not unconditionally recommended. 

One final use of deviance residuals is to see whether there are any high leverage points in our model. We want to examine the deviance residuals and see which ones are abnormally large. It may be worth examining those points to see whether there is anything odd about them. Let's consider our running example. We plot the residuals versus the predictors and versus the predicted values, to see if we have any trends. 

```{r}
plot(df_zero$x, df_zero$deviance_residuals)
plot(df_zero$fitted_values, df_zero$deviance_residuals)
```

We see that there are two points that seem to have large residuals. Let's examine the two points.

```{r}
which.max(df_zero$deviance_residuals)
which.min(df_zero$deviance_residuals)
df_zero$color <- factor(ifelse(1:200 %in% c(7, 195), 2, 1))
ggplot(df_zero, aes(x, y, color = color)) + geom_point()
```

And they are the two points that we would have expected from the plot! We can also pull out the leverage of the points. If we plot them against the predictor, we get a typical looking plot. Note that the leverage tends to be very small where the probability of success is close to zero or one, as well as in the middle. There is usually this bimodal distribution.

```{r}
plot(df_zero$fitted_values, influence(mod)$hat)
```

We can see what the most influential point is. 

```{r}
which.max(influence(mod)$hat)
df_zero$color <- factor(ifelse(1:200 == 36, 2, 1))
ggplot(df_zero, aes(x, y, color = color)) + geom_point() 
```

Kind of a surprising value to have the maximum influence! As with OLS, we can find high leverage outliers via the `plot` function. We see that the same two points with highest deviance residuals are the ones that have the most leverage.

```{r}
plot(mod, which = 5)
```

### Case Study

Let's consider a more realistic data set that isn't just simulated data. The `MASS` package contains a data frame called `birthwt`, which is 189 births in the US. We are trying to model low birth weight, which is classified as less than 2.5kg.

```{r}
bwt <- MASS::birthwt
summary(bwt)
```

While the actual birthweight is available, we are certainly not allowed to use that in our model! Looking at the help page, we make the following changes.

```{r}
bwt$race <- factor(bwt$race, levels = 1:3, labels = c("white", "black", "other"))
bwt$smoke <- factor(bwt$smoke, levels = 0:1, labels = c("No", "Yes"))
```

We make a coupld of other changes. We change `ftv` to be a simple yes/no did the mother visit a physician in the first trimester. Simlarly, we use whether a woman has had a premature labor as an indicator.

```{r}
bwt$ftv <- factor(ifelse(bwt$ftv > 0, "Yes", "No"))
bwt$ptl <- factor(ifelse(bwt$ptl > 0, "Yes", "No"))
bwt$ht <- factor(bwt$ht, levels = 0:1, labels = c("No", "Yes"))
bwt$ui <- factor(bwt$ui, labels = c("No", "Yes"))
summary(bwt)
```

Let's build our first model.

```{r}
mod <- glm(low ~ .-bwt, data = bwt, family = "binomial")
summary(mod)
```

We see that the residual deviance is lower than the null deviance. We see that, moeroever, the residual deviance is roughly the same as its degree of freedom, so we have no serious reason to question the fit. We now remove some of the variables, using `stepAIC`.

```{r}
mod_reduced <- MASS::stepAIC(mod, trace = 0)
summary(mod_reduced)
```

Now, let's consider interactions. We start with the variables that we already have as a lower model, then we have as an upper model all interactions, together with age and lwt squared. Note that doing it this way keeps us from getting fitted probabilities of 0/1, which is generally better to avoid. That being said, I am not sure I would have thought to do this! This is how Venables and Ripley did it in their book. I recommend that you run this code below in your own terminal with `trace = 1` so that you can see what models it was considering.

```{r}
mod2_reduced <- MASS::stepAIC(mod, ~ .^2 + I(scale(age)^2) + I(scale(lwt)^2), trace = 0)
```

Now, we see what percentage of observations in the data set are misclassified.

```{r}
bwt$fitted_values <- predict(mod2_reduced, type = "response")
bwt$prediction <- ifelse(bwt$fitted_values > 1/2, 1, 0)
mean(bwt$prediction == bwt$low)
```

The plots of residuals versus fitted and leverage don't indicate any problems.

```{r}
plot(mod2_reduced, which = 5)
plot(mod2_reduced, which = 1)
```

## Count response models

In this section, we discuss models when the response is a count variable. That is, the response is an integer that can at least be interpreted as the count of some number of items. Examples might include the number of days of hospital stays for patients, the number of cigarettes smoked on a given day, or the number of likes that a tweet receives. 

We start by reviewing two distributions that will come up in this section. A random variable $X$ is a *Poisson* with rate $\lambda$ if $X$ is a discrete rv and has probability mass function given by

\[
P(X = x) = e^{-\lambda} \frac{\lambda^x}{x!} \qquad x = 0, 1, \ldots
\]

Recall that we use Poisson rv's to model the number of occurrences in a Poisson process over some time interval, such as the number of accidents at an intersection over a month or the number of typos in one page of typing. For more review, see [here](https://mathstat.slu.edu/~speegle/_book/randomvariables.html#other-special-random-variables). 

Now, we imagine that we have data of the form $({\bf {x}}_1, y_1), \ldots, ({\bf x}_n, y_n)$, where $y_i$ are the responses, and ${\bf x}_i$ are the predictors. We also imagine that the $y_i$ are nonnegative integers, and we suspect that they could be random samples from a Poisson random variable, whose mean $\lambda$ depends on the values of the predictors. We model the relationship between predictors and $\lambda$ as

\[
\log \lambda = \beta_0 + \sum_{i = 1}^P \beta_i x_i
\]

The **random component** of the response is Poisson with $\lambda = e^{\beta_0 + \sum_{i = 1}^P \beta_i x_i}$. The*link function* is $\log x$. This is the link between the linear combination of predictors and the expected value of the response, while the inverse link function is $e^x$. 

Let's set up some simulated data. Note that we are choosing the data to follow precisely a Poisson regression model with one predictor and $\beta_0 = 2$, $\beta_1 = 1$.

```{r}
x <- runif(100, 0, 2)
y <- rpois(100, lambda = exp(2 + x))
dd <- data.frame(x = x, y = y)
plot(x, y)
```

Looking at the plot, we see that the variance of the response depends on the mean of the response! That is because the response is Poisson, and the mean of a Poisson with rate $\lambda$ is $\lambda$, while the standard deviation is $\sqrt{\lambda}$. This is an important observation. In order for a Poisson regression model to be accurate, the mean of the response should also be the variance. From a practical point of view, if you are looking at a graph of data, then the range of values near where the mean of the response is $y_0$ should be about $y_0 \pm 2\sqrt{y_0}$. We will see more detailed ways of checking this assumption later in the chapter.

Next, we want to estimate the parameters of the model. In this case, $\beta_0$ and $\beta_1$. Let's start by minimizing the SSE, like we did in OLS.

```{r}
sse <- function(beta, dat) {
  sum(abs(dat$y - exp(beta[1] + beta[2] * dat$x))^2)
}
optim(par = c(1, 1), fn = sse, dat = dd)
```

We see that the estimates of $\beta_0 = 1.9415$ and $\beta_1 = 0.9943$ are pretty accurate! As before, we can modify the error function so that it is more or less sensitive to outliers, say by replacing it with a Huber function or a support vector function. These lead to different estimates for the parameters, which we could chose from based on cross-validation, if we were so inclined. In this chapter, however, we are covering the more classical theory, which uses Maximum Likelihood to estimate the parameters. As we will see, while minimizing SSE can give something pretty close to the maximum likelihood estimators, it is not exactly the same thing.

Recall that the likelihhod function is the product of the likelihood functio of the model, evaluated at the data points. In this case, it is

\[
\Pi\ \text{dpois}(y_i, e^{\beta_0 + \beta_1 x_i})
\]

As before, we will take the log of this so that we don't have issues with overflow. The function that we are maximizing is:

\[
\sum \log \ \text{dpois}(y_i, e^{\beta_0 + \beta_1 x_i})
\]

```{r}
neg_log_likelihood <- function(beta, dat) {
  -sum(log(dpois(dat$y, exp(beta[1] + beta[2] * dat$x))))
}
optim(par = c(1,1), fn = neg_log_likelihood, dat = dd)
```

We get very similar estimates for $\beta_0$ and $\beta_1$, but not exactly the same. Let's plot the expected value of $Y$ versus $x$ along with the data.

```{r}
beta <- optim(par = c(1,1), fn = neg_log_likelihood, dat = dd)$par
plot(x, y)
curve(exp(beta[1] + beta[2] * x), add = T)
```

Looks like a pretty good fit to me! 

The built in R way of doing this is to use `glm` with `family = "poisson"`. 

```{r}
mod <- glm(y ~ x, data = dd, family = "poisson")
summary(mod)
```

The residual deviance plays a similar role in Poisson regression to what it played in logistic regression. We see that we have reduced the deviance from the null deviance by quite a large margin, so we are explaining a good deal of the variance through the model. There is one aspect, however, where residual deviance plays an expanded role in Poisson regression. We use it to detect **overdispersion**.

Recall that an important aspect of our model was that we are assuming that the response is Poisson, so the variance of the response should increase with the mean of the response. This is something that we really want to check. The data exhibits overdispersion if the variance is larger than the mean, and underdispersion if the variance is smaller than the mean. Underdispersion is generally less of a problem than overdispersion. 

Let's see how to check overdispersion. We first want to see what the distribution of the residual deviance will be under the null hypothesis. So, we replicate the entire process of sampling data, building a model and computing the residual deviance.

```{r}
res_dev <- replicate(10000, {
  x <- runif(100, 0, 2)
  y <- rpois(100, lambda = exp(2 + x))
  dd <- data.frame(x = x, y = y)
  mod <- glm(y ~ x, data = dd, family = "poisson")
  deviance(mod) #returns the residual deviance
})
```

Now, let's plot it.

```{r}
plot(density(res_dev))
```

Looks plausibly normal, but probably has some skewness. So let's compare to a $\chi^2$.

```{r}
plot(density(res_dev))
curve(dchisq(x, df = 98), add = T, col = 2)
```

To test for overdispersion, we would compare the observed residual deviance to a $\chi^2$ with $n - 2$ degrees of freedom. If the result is unlikely, we conclude that the model is not a good fit, at least possibly because of overdispersion.

```{r}
pchisq(deviance(mod), df = 98, lower.tail = F)
```

For the data that we have here, we would not reject the null hypothesis, and ther is no reason to believe that the model is fitting poorly. Let's look an example that **does** exhibit overdispersion.

```{r}
set.seed(4212020)
rqpois = function(n, lambda, phi) {
  mu = lambda
  k = mu/phi/(1-1/phi)
  rnbinom(n, mu = mu, size = k)
}
dd <- data.frame(x = runif(100, 0, 2))
dd <- mutate(dd, y = rqpois(100, exp(2 + x), 4))
plot(dd)
```

The overdispersion is not obvious in the plot, but notice that when the mean is about 50, the range of values of the response is about 40, which indicates a standard deviation closer to 10 than to the square root of 50. 

```{r}
mod <- glm(y ~ x, data = dd, family = "poisson")
summary(mod)
```

Note that the residual deviance is quite larger than its degrees of freedom! That is an indicator of overdispersion. We run the test based on the $\chi^2$ statistic:

```{r}
pchisq(deviance(mod), 98, lower.tail = F)
```

We see that we would reject the null hypothesis that the model fits the data. One possible reason for this could be overdispersion, but this is not in and of itself a specific test for overdispersion. The [test](https://www.sciencedirect.com/science/article/abs/pii/030440769090014K) below, based on work by Cameron and Trivedi, is specifically designed to detect overdispersion.

This tests $H_0: \alpha = 0$ versus $H_a: \alpha > 0$ in the model for variance of $\sigma^2 = \mu + \alpha \mu$. 

```{r}
library(AER)
dispersiontest(mod, trafo = 1)
```

We see that we reject the null hypothesis in favor of the alternative, and we estimate that the  relationship between variance and the mean is $\sigma^2 \approx 3\mu$. In particular, Poisson regression is inappropriate for this data set.




[^1]: Extra credit: diagram the sentence "The bigger the value of the log odds, the better."

[^2]: Pronounced: "shoe!"





