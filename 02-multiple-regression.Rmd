# Multiple Linear Regression {#multiple_regression}

In this chapter we consider the case where we have one response variable $y$ and multiple predictor variables $x_1, \ldots, x_K$. Our model is
\[
y = \beta_0 + \sum_{k = 1}^K \beta_k x_k + \epsilon
\]
where $\epsilon$ is normal with mean 0 and standard deviation $\sigma^2$. Note that this notation is equivalent to 
\[
y = \beta_0 + \beta_1 x + \epsilon
\]
in the single predictor case. Let's restrict down to the case where we have *three* predictors, which is easier notationally. 

Suppose we have data that consists of one response variable and three predictor variables, $v, w$ and $x$. We are interested in the model

\[
y = \beta_0 + \beta_1 v + \beta_2 w + \beta_2 x + \epsilon
\]

where $\epsilon$ is a normal random variable with mean 0 and variance $\sigma^2$. Given data $(v_1, w_1, x_1, y_1), \ldots, (v_n, w_n, x_n, y_n)$, we compute the $\hat \beta_i$ by minimizing the SSE,

\[
SSE = \sum_{i = 1}^n \bigl(y_i - (\beta_0 + \beta_1 v_i + \beta_2 w_i + \beta_3 x_i)\bigr)^2
\]

The R function `lm` will do this for you. Let's look at some simulated data.

```{r}
set.seed(1262020)
vs <- runif(30, 0, 10)
ws <- runif(30, 0, 10)
xs <- runif(30, 0, 10)
dd <- data.frame(vs,
                 ws,
                 xs,
                 ys = 1 + 2* vs + 3 * ws + rnorm(30, 0, 5))
mod1 <- lm(ys ~., data = dd)
mod1
```

We can also do hypothesis tests of $\beta_i = 0$ versus $\beta_i \not= 0$ by looking at the summary.

```{r}
summary(mod1)
```

We see that we reject that $\beta_1$ and $\beta_2$ are 0, but fail to reject that $\beta_0$ and $\beta_3$ are zero, at the $\alpha = .05$ level. 

Our primary goal is going to do **variable selection**. Many times, we have more predictors than we really need, and several of them do not contribute to the model. We will remove those predictors in order to form a simpler and more understandable model. We will also see that removing predictors leads to better prediction for future values!

Based on the above, we might consider removing `xs` from the  model, and re-computing.

```{r}
mod2 <- lm(ys ~ vs + ws, data = dd)
summary(mod2)
```
Our model would then be $y = -2.9041 + 2.4214 v + 2.8949 w + \epsilon$, at which point we would probably be done removing variables. 

Let's look at another example. Consider the data in `cystfibr` in the `ISwR` package. Our treatment of this data set follows the treatment given in Dalgaard's book. Let's start by seeing what kind of data we are dealing with. According to the description, "It contains lung function data for cystic fibrosis patients (7â€“23 years old)." I recommend running `?ISwR::cystfibr` and reading about the variables involved in this data. 

We begin by plotting and summarizing the data.

```{r}
cystfibr <- ISwR::cystfibr
summary(cystfibr)
```

There's not a whole lot to see here. `frc`, `weight` and `pemax` appear to be right skew, and `sex` has been miscoded as numeric rather than as a factor. Let's recode.

```{r}
cystfibr$sex <- factor(cystfibr$sex, levels = c(0, 1), labels = c("Male", "Female"))
summary(cystfibr$sex)
```

Now, let's do a scatterplot.

```{r}
plot(cystfibr)
```

That's pretty spectacular, but not very informative. It looks like `height` and `weight` are related, as well as `age`. The variable `pemax`, which is the variable of interest, appears to be correlated with some of the other variables, but is is hard to tell. Here is a tidy plot. We use the original `ISwR::cystfibr` because in this case, we don't want `sex` coded as a factor.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
ISwR::cystfibr %>% tidyr::pivot_longer(cols = -pemax,
                                 names_to = "variable",
                                 values_to = "value") %>%
  ggplot(aes(x = value, y = pemax)) + geom_point() +
    facet_wrap(facets = ~variable, scales = "free_x") 
```



Wait, what do I mean by correlated???

We define the **covariance** of $X$ and $Y$ to be 
\[
E[(X - \mu_X) (Y - \mu_Y)] = E[XY] - E[X]E[Y],
\]
where $\mu_X$ is the mean of $X$ and $\mu_Y$ is the mean of $Y$.
Looking at the first expression, we see that $(X - \mu_X) (Y - \mu_Y)$ is positive exactly when *either* both $X$ and $Y$ are bigger than their means, *or* when both $X$ and $Y$ are less than their means. Therefore, we expect to see a positive covariance if increased values of $X$ are probabilistically associated with increased values of $Y$, and we expect to see a negatice covariance when increased values of $X$ are associated with decreased values of $Y$.

The **correlation** of $X$ and $Y$ is given by
\[
\rho(X, Y) = \frac{E[(X - \mu_X) (Y - \mu_Y)]}{\sigma_X \sigma_Y}
\]
The correlation is easier to interpret in most cases than the covariance, because the values are between $-1$ and $1$. A value of $\pm 1$ corresponds to a perfect linear relationship between $X$ and $Y$, and the larger tha absolute value of the correlation is, the stronger we expect the linear relationship of $X$ and $Y$ to be.

We can estimate the covariance and correlation using the built in R functions `cov` and `cor`. If we apply `cor` to a data frame, then it computes the correlation of each pair of variables separately.

```{r}
cor(select(cystfibr, -sex))
```

Indeed, we see that `age`, `height` and `weight` are pretty strongly correlated, as are some of the lung function variables. `pemax` is not highly correlated with any one other variable.



Let's run multiple regression on all of the variables at once, and see what we get:

```{r}
my.mod <- lm(pemax~., data = cystfibr)
summary(my.mod)
```

A couple of things to notice: first, the p-value is 0.03195, which indicates that the variables *together* do have a statistically significant predictive value for `pemax`. However, looking at the list, no one variable seems necessary. The reason for this is that the $p$-values for each variable is testing what happens when we remove *only* that variable. So, for example, we can remove weight and not upset the model too much. Of course, since height is strongly correlated with weight, it may just be that height will serve as a proxy for weight in the event that weight is removed!

We start to eliminate variables that are not statistically significant in the model. We will begin with the other lung function data, for as long as we are allowed to continue to remove. The order chosen for removal is: tlc, frc,  rv, fev1.

```{r}
summary(lm(pemax~height+age+weight+sex+bmp+fev1+rv+frc, data = cystfibr))
summary(lm(pemax~height+age+weight+sex+bmp+fev1+rv, data = cystfibr))
summary(lm(pemax~height+age+weight+sex+bmp+fev1, data = cystfibr))
summary(lm(pemax~height+age+weight+sex+bmp, data = cystfibr))
```

When removing variables in a stepwise manner such as above, it is important to consider adding variables back into the model. Since we have removed all of the lung function data one variable at a time, let's see whether the **group** of lung function data is significant as a whole. This serves as a sort of test as to whether we need to consider adding variables back in to the model.

```{r}
mod1 <- lm(pemax~., data = cystfibr)
mod2 <- lm(pemax~height+age+weight+sex+bmp, data = cystfibr)
anova(mod2, mod1)
```
Since the $p$-value is 0.4758, this is an indication that the group of variables is not significant as a whole, and we are at least somewhat justified in removing the group.


Returning to variable selection, we see that age, height and sex don't seem important, so let's start removing.
```{r}
summary(lm(pemax~height+weight+sex+bmp, data = cystfibr))
summary(lm(pemax~weight+sex+bmp, data = cystfibr))
summary(lm(pemax~weight+bmp, data = cystfibr))
```

Again, we have removed a group of variables that are related, so let's see if the group as a whole is significant. If it were, then we would need to consider adding variables back in to our model.
```{r}
mod3 <- lm(pemax~weight+bmp, data = cystfibr)
anova(mod3, mod2)
```

This indicates that we are justified in removing the age, weight and sex variables. And now, bmp also doesn't seem important. We are left with modeling pemax on weight, similar to what we had before. 


```{exercise}
Consider the data set `ISwR::secher`. Read about the data set by typing `?ISwR::secher`. We are interested in predicting birth weight on the other variables. 

  a. Should any of the variables be removed immediately without doing any testing?
  
  b. Find a minimal meaningful model that explains birthweight in terms of other variable(s).   
  
  c. Check the residuals.
```

### Aside on multiple R squared and ANOVA {#anova}

The Multiple R Squared value is 0.6373. This means that the model explains 63% of the variance of pemax. To better understand what that means, let's see how we could calculate it. First, we compute the total variance of `pemax`.

```{r}
pemax_var <- var(cystfibr$pemax)
```

Now, we compute the variance of the **residuals** of our model. We can think of the variance of the residuals as the variance in `pemax` that remains unexplained after taking into account the variables in our model. Notice that we don't correct for the number of parameters we are estimating.

```{r}
residuals_var <- var(my.mod$residuals)
```

So, the percentage of variance explained is:

```{r}
(pemax_var - residuals_var)/pemax_var
```



What would have happened if we had used the variance adjusted for the number of parameters that we are estimating? 

```{r}
residuals_var_corrected <- 1/(25 - (9 + 1)) * sum(my.mod$residuals^2)
(pemax_var - residuals_var_corrected)/pemax_var
```

In this case, we get the **adjusted** $R^2$! Whoa! 

```{exercise}
In this exercise, you will see evidence that adding variables always increases the multiple R squared, but that adding noise to a model generally decreases the adjusted R squared. Define `cystifbr$random <- rnorm(25)`. 

    a. Compute the Multiple R-squared when `pemax` is modeled on all of the predictors. How does it compare to 0.6373354? You may need to compute it out to many digits to see a difference! 
      
    b. Repeat for the adjusted R-squared. How does it compare to 0.4197366?

```

Moving on, we said above that the variables **taken together** have a $p$-value of 0.03195. What does that mean, exactly?

Let's consider the case where we have three predictors and one response, to make things a bit simpler. Our null hypothesis would be that there is no linear association between the three predictors and the response. In other words, if our model is $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \epsilon$, then the null hypothesis is that $\beta_1 = \beta_2 = \beta_3 = 0$, and the alternative hypothesis is that not all of them are zero. 

As we saw above, the multiple R squared will never be zero, but we ask the question: how big is big enough that we can be pretty sure that it isn't due to chance? We could do it directly via simulations, but we are supposed to be doing classical theory now! So, let's see what kind of distribuytions the component pieces have.

We start with the estimate of $\sigma^2$ based on the residuals.

```{r}
set.seed(1262020)
x1s <- runif(30, 0, 10)
x2s <- runif(30, 0, 10)
x3s <- runif(30, 0, 10)
dd <- data.frame(x1s = x1s,
                 x2s = x2s,
                 x3s = x3s,
                 ys = 1 + 2 * x1s + 3 * x2s + rnorm(30, 0, 10) 
                 )
summary(lm(ys ~ ., data = dd))
```

Based on our previous work, we would expect an **unbiased** estimator for $\sigma^2$ to be the sum-squared residuals over (30 - 4), where 4 is the number of parameters that we estimated. Let's see if that is right. First we check that the esimate of the variance $\sigma^2$ in the summary really is computed by dividing by 26.

```{r}
random.mod <- lm(ys ~ ., data = dd)
summary(random.mod)$sigma^2
1/26*sum(random.mod$residuals^2)
```
Yep. Now, let's see that it appears to be an unbiased estimate for $\sigma^2$.

```{theorem}
Let $\hat \epsilon_i$ be the residuals when $y = \beta_0 + \sum_{k = 1}^K \beta_k x_k + \epsilon$, with $\epsilon \sim N(0, \sigma^2)$. Then
$\frac{1}{n - (K + 1)}\sum_{i = 1}^n \hat \epsilon^2$ is an unbiased estimator for $\sigma^2$. 
```

We denote the estimator $\hat \sigma^2 = \frac{1}{n - (K + 1)}\sum_{i = 1}^n \hat \epsilon^2$.

```{r, cache=TRUE}
sim_data <- replicate(10000, {
  dd <- data.frame(x1s = x1s,
                 x2s = x2s,
                 x3s = x3s,
                 ys = 1 + 2 * x1s + 3 * x2s + rnorm(30, 0, 10) 
                 )
  summary(lm(ys ~ ., data = dd))$sigma^2
})
mean(sim_data)
```
Yep. (Repeat the above code a few times to see that it appears to be centered at 100.) What kind of rv is our estimate for $\sigma^2$? We can adjust it to be $\chi^2$!

```{r}
plot(density(sim_data * 26/100))
curve(dchisq(x, 26), add = T, col = 2)
```

To summarize, we have the following theorem.

```{theorem}
Let $\hat \epsilon_i$ be the residuals when $y = \beta_0 + \sum_{k = 1}^K \beta_k x_k + \epsilon$, with $\epsilon \sim N(0, \sigma^2)$, and let $\hat \sigma^2$ denote the estimator for $\sigma$ discussed above. Then, $\frac{n - (K + 1) \hat \sigma^2}{\sigma^2} \sim \chi^2_{n - (K + 1)}$. That is,  $\frac{n - (K + 1) \hat \sigma^2}{\sigma^2}$ is a $\chi^2$ random variable with $n - (K + 1)$ degrees of freedom.
```


Next, we assume that the null is true. We can estimate the variance as follows, which we denote as $V(y)$. 

```{r}
sim_data_2 <- replicate(10000, {
  ys = rnorm(30, 0, 10) 
  sum((ys - mean(ys))^2)/29
})
plot(density(sim_data_2 * 29 / 100))
curve(dchisq(x, 29), add = T, col = 2)
```

We see that the variance estimate in this case is a $\chi^2$ random variable with 29 degrees of freedom. Now the cool part. What about the difference between the variance we computed both ways?? Again, let's assume that the null hypothesis is `TRUE`

```{r, cache=TRUE}
sim_data_3 <- replicate(10000, {
    dd <- data.frame(x1s = x1s,
                 x2s = x2s,
                 x3s = x3s,
                 ys = rnorm(30, 0, 10) 
                 )
    sum((dd$ys - mean(dd$ys))^2)/29 * 29/100 -summary(lm(ys ~ ., data = dd))$sigma^2 * 26/100 
      
})
plot(density(sim_data_3))
curve(dchisq(x, 3), add = T, col = 2)
```

Finally, we take the ratio, similar to the multiple R squared.

```{r, cache=TRUE}
sim_data_3 <- replicate(10000, {
    dd <- data.frame(x1s = x1s,
                 x2s = x2s,
                 x3s = x3s,
                 ys = rnorm(30, 0, 10) 
                 )
    ((sum((dd$ys - mean(dd$ys))^2)/100 -summary(lm(ys ~ ., data = dd))$sigma^2 * 26/100)/3)/ (summary(lm(ys ~ ., data = dd))$sigma^2 * 26/100/26)
})
plot(density(sim_data_3))
curve(df(x, 3, 26), add = T, col = 2)
```

We again summarize the above in the following theorem.

```{theorem}
With the notation from this section,

a. $\frac{(n - 1)V(y)}{\sigma^2} - \frac{n - (K + 1)\hat \sigma^2}{\sigma^2} \sim \chi^2_K$.

b. $F = \frac{\bigl((n - 1)V(y) - (n - (K + 1)) \hat \sigma^2\bigr)/K}{\bigl((n - (K + 1))\hat \sigma^2\bigr)/(n - (K + 1))} \sim F_{K, n - (K + 1)}$

```

Note that **large** values of $F$ correspond to evidence **against** the null hypothesis and in favor of the alternative hypothesis. Therefore, we will compute $F$ and reject the null hypothesis in favor of the alternative if it is larger than `qf(.95, K, (n - (K + 1)))`. We can compute the $p$ value of `pf(F, K, n - (K + 1), lower.tail = FALSE)`. Let's see it in action.

```{r}
set.seed(1262020)
x1s <- runif(30, 0, 10)
x2s <- runif(30, 0, 10)
x3s <- runif(30, 0, 10)
dd <- data.frame(x1s = x1s,
                 x2s = x2s,
                 x3s = x3s,
                 ys = 1 + 2 * x1s + 3 * x2s + rnorm(30, 0, 10) 
                 )
mod <- lm(ys ~ ., data = dd)
vy <- var(dd$ys)
sigmahat <- sum(mod$residuals^2)/(30 - (3 + 1))
Fstat <- ((29 * vy - 26 * sigmahat)/3)/(26 * sigmahat/26)
pf(Fstat, 3, 26, lower.tail = FALSE)
mod <- lm(ys ~ ., data = dd)
summary(mod)
```

We note that the $F$ statistic and the $p$-value match what `lm` gives us. 

```{exercise}
For this exercise, use the data frame `dd` that we created in the previous code chunk. Suppose that we know that $\beta_1 \not= 0$, and we wish to test $H_0: \beta_2 = \beta_3 = 0$ versus $H_a$: at least one of $\beta_2$ and $\beta_3$ is not zero. Repeat the analysis above to do this hypothesis test. The outline of what you need to do is given below.
```

#### Hint to Exercise

a. Build a model $y = \beta_0 + \beta_1 x_1 + \epsilon$, and estimate $\sigma^2$ based on this model. This estimate will play the role of $V(y)$ in the previous section. Think about how many parameters you are estimating to get an unbiased estimator. Call this estimator $\hat \sigma_1^2$.
    
b. Show that $(n - 2) \hat \sigma_1^2/\sigma^2$ is $\chi^2$ with $n -2$ degrees of freedom.
    
c. Use the same estimate for $\hat \sigma^2$ as we did above.
    
d. Show that $\frac{\bigl(n - 2)\hat \sigma_1^2) - (n - (K + 1) \hat \sigma^2)\bigr)/2}{(n - (K + 1) \hat \sigma^2)/26} \sim F_{2, 26}$. 
    
e. Use your result in d. to do the hypothesis test. If you do everything correctly, your $p$-value will be about 0.001258.


## More Variable Selection: ANOVA


Let's look at an ANOVA table:

```{r}
anova(my.mod)
```

We get a different result, because in this case, the table is telling us the significance of adding/removing variables one at a time. For example, age is significant all by itself. Then R compares the model with `age` and `sex` to the model with just `age`, just like you did in the exercise in the previous section. Then it compares the model with `age`, `sex` and `height` to the model with just `age` and `sex`. Once age is added, sex, height, etc. are not significant. Note that bmp is close to the mystical, magical .05 level^[that's a joke. it **is** close to .05, but there is nothing magical about .05.]. However, we are testing 9 things, so we should expect that relatively often we will get a false small p-value. 

One way to test this further is to see whether it is permissible to remove *all* of the variables once age is included. We do that using anova again, and this is a short-cut to what you did in the exercise in the previous section!

```{r}
my.mod2 <- lm(pemax~age, data = cystfibr)
anova(my.mod2, my.mod) #Only use when models are nested! R doesn't check
```

As you can see, the p-value is 0.2936, which is not significant. Therefore, we can reasonably conclude that once age is included in the model, that is about as good as we can do. *Note:* This does NOT mean that age is necessarily the best predictor of pemax. The reason that it was included in the final model is largely due to the fact that it was listed first in the data set! 

Let's see what happens if we change the order:

```{r}
my.mod <- lm(pemax~height+age+weight+sex+bmp+fev1+rv+frc+tlc, data = cystfibr)
summary(my.mod)
```

Note that the overall p-value is the same.

```{r}
anova(my.mod)
```

Here, we see that height is significant, and nothing else. If we test the model pemax ~ height versus pemax ~ ., we get

```{r}
anova(my.mod2, my.mod) #Only use when models are nested!
```

And again, we can conclude that once we have added height, there is not much to be gained by adding other variables. We leave it to the reader to test whether weight follows the same pattern as height and age.

Let's first examine a couple of other ways we could have proceeded.

```{r}
my.mod <- lm(pemax~., data = cystfibr)
summary(my.mod)
```

Let's take a look at the residuals.


```{r}
my.mod <- lm(pemax~height, data = cystfibr)
plot(pemax~height, data = cystfibr)
abline(my.mod)
plot(my.mod)
```

## Predictions

Let's return to the `cystfibr` data set and consider predictions. Suppose that a new patient presents, and we measure all of their data except `pemax`, and we want to predict their `pemax`. 

```{r echo=FALSE}
set.seed(1282020)
new_patient <- cystfibr[sample(1:25, 1),]
sds <- sapply(3:10, function(x) sd(cystfibr[,x]))
new_patient[, 3:10] <- round(new_patient[,3:10] + rnorm(8, 0, sds/2), 1)
new_patient
```

We can take our final model of `pemax` described by `weight` and use that.

```{r}
mod <- lm(pemax ~ weight, data = cystfibr)
predict(mod, newdata = new_patient, interval = "predict")
```
We have an estimate of 146 with a range of possible values from 87 to 205.

What if we had used height?

```{r}
mod2 <- lm(pemax ~ height, data = cystfibr)
predict(mod2, newdata = new_patient, interval = "predict")
```
What? This gives us a very different answer! This gives us 116 with a range from 59 to 174. We have kind of said above that from a modeling point of view, it is hard to pick one model over the other. But from a **predictive** point of view, they are giving us very different answers. How can we decide which one is better at predicting future values? That is the topic of the next chapter!



## Exercises

1. Consider the data set `ISwR::secher`. Read about the data set by typing `?ISwR::secher`. We are interested in predicting birth weight on the other variables. 

    a. Should any of the variables be removed immediately without doing any testing?
  
    b. Find a minimal meaningful model that explains birthweight in terms of other variable(s).   
  
    c. Check the residuals.
  
    &nbsp;
1. In this exercise, you will see evidence that adding variables always increases the multiple R squared, but that adding noise to a model generally decreases the adjusted R squared. Consider the `cystfibr` data set from the section. Define `cystifbr$random <- rnorm(25)`. 

    a. Compute the Multiple R-squared when `pemax` is modeled on all of the predictors. How does it compare to 0.6373354? You may need to compute it out to many digits to see a difference! 
      
    b. Repeat for the adjusted R-squared. How does it compare to 0.4197366?

    &nbsp;
2. For this exercise, use the data frame `dd` that we created with the following code:
```{r}
set.seed(1262020)
x1s <- runif(30, 0, 10)
x2s <- runif(30, 0, 10)
x3s <- runif(30, 0, 10)
dd <- data.frame(x1s = x1s,
                 x2s = x2s,
                 x3s = x3s,
                 ys = 1 + 2 * x1s + 3 * x2s + rnorm(30, 0, 10) 
                 )
```
Suppose that we know that $\beta_1 \not= 0$, and we wish to test $H_0: \beta_2 = \beta_3 = 0$ versus $H_a$: at least one of $\beta_2$ and $\beta_3$ is not zero. Repeat the analysis in Section \@ref(anova), *mutatis mutandis*. The outline of what you need to do is given in the section.

